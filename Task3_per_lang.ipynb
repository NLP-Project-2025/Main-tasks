{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a406f4ab-f2f7-4b0a-9f69-9ec89a6821db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T06:36:52.630817Z",
     "iopub.status.busy": "2025-12-06T06:36:52.630008Z",
     "iopub.status.idle": "2025-12-06T06:36:56.412446Z",
     "shell.execute_reply": "2025-12-06T06:36:56.411494Z",
     "shell.execute_reply.started": "2025-12-06T06:36:52.630780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.1.1+cu121)\n",
      "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.35.2)\n",
      "Collecting einops>=0.6.0 (from -r requirements.txt (line 3))\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.26.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.7.3)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.66.1)\n",
      "Collecting prettytable (from -r requirements.txt (line 8))\n",
      "  Downloading prettytable-3.17.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.5.0->-r requirements.txt (line 6)) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->-r requirements.txt (line 6)) (2023.4)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->-r requirements.txt (line 8)) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prettytable-3.17.0-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: prettytable, einops\n",
      "Successfully installed einops-0.8.1 prettytable-3.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2903822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:20:27.789622Z",
     "iopub.status.busy": "2025-12-06T10:20:27.789374Z",
     "iopub.status.idle": "2025-12-06T10:20:30.434286Z",
     "shell.execute_reply": "2025-12-06T10:20:30.433656Z",
     "shell.execute_reply.started": "2025-12-06T10:20:27.789604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn, dtype\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from config import PretrainedConfig, GPT2Config\n",
    "from transformers import GPT2Model as OpenAIGPT2Model\n",
    "from transformers import GPT2Tokenizer\n",
    "from utils import *\n",
    "from einops import rearrange\n",
    "from typing import Callable, Iterable, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f3b1ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T02:11:17.144246Z",
     "iopub.status.busy": "2025-12-06T02:11:17.143971Z",
     "iopub.status.idle": "2025-12-06T02:11:17.149193Z",
     "shell.execute_reply": "2025-12-06T02:11:17.148709Z",
     "shell.execute_reply.started": "2025-12-06T02:11:17.144231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /notebooks\n",
      "Files in current directory: ['requirements.txt', 'model_en_epoch_3.pt', 'XNLI-1.0', '.Trash-0', 'pretrain.txt', 'best_model', 'optimizer_test.npy', 'Task3_part 1.ipynb', '.git', 'utils.py', '__pycache__', 'task4.ipynb', 'toy_gpt2_model_full.pth', 'config.py', '.ipynb_checkpoints', 'XNLI-MT-1.0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check current directory\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Files in current directory:\", os.listdir('.'))\n",
    "\n",
    "# If XNLI folders aren't visible, you might need to change directory\n",
    "# Uncomment and adjust if needed:\n",
    "# os.chdir('/path/to/your/project/folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23384a93",
   "metadata": {},
   "source": [
    "## Task 3 Fine-tune GPT-2 (per-language)\n",
    "\n",
    "Guidance: Load the pretrained GPT-2 (not the ones fine-tuned on English NLI) along with the training data for a single target language. Choose non-English languages that performed well in the zero-shot cross-lingual transfer and fertility evaluation. It depends on you how many languages to include. Fine-tune a separate model for each selected language. Afterwards, compare these per-language fine-tuned models with the zero-shot cross-lingual transfer results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e498552",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb30c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:20:47.313364Z",
     "iopub.status.busy": "2025-12-06T10:20:47.313012Z",
     "iopub.status.idle": "2025-12-06T10:20:47.429498Z",
     "shell.execute_reply": "2025-12-06T10:20:47.428951Z",
     "shell.execute_reply.started": "2025-12-06T10:20:47.313353Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# text = \"Welcome, this is the beginning of default final project!\"\n",
    "# input_ids = tokenizer(text)['input_ids']\n",
    "# print('input_ids:', input_ids)\n",
    "# for token in input_ids:\n",
    "#     print('token', tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd74b99",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "061db6ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:22:32.452367Z",
     "iopub.status.busy": "2025-12-06T10:22:32.452139Z",
     "iopub.status.idle": "2025-12-06T10:22:32.459496Z",
     "shell.execute_reply": "2025-12-06T10:22:32.458895Z",
     "shell.execute_reply.started": "2025-12-06T10:22:32.452351Z"
    }
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_attention_heads = config.num_attention_heads\n",
    "    self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "    self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "    # Initialize the linear transformation layers for key, value, query.\n",
    "    self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "    # This dropout is applied to normalized attention scores following the original\n",
    "    # implementation of transformer. Although it is a bit unusual, we empirically\n",
    "    # observe that it yields better performance.\n",
    "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "  def transform(self, x, linear_layer):\n",
    "    # The corresponding linear_layer of k, v, q are used to project the hidden_state (x).\n",
    "    proj = linear_layer(x)\n",
    "    # Next, we need to produce multiple heads for the proj. This is done by spliting the\n",
    "    # hidden state to self.num_attention_heads, each of size self.attention_head_size.\n",
    "    proj = rearrange(proj, 'b t (h d) -> b t h d', h=self.num_attention_heads)\n",
    "    # By proper transpose, we have proj of size [bs, num_attention_heads, seq_len, attention_head_size].\n",
    "    proj = rearrange(proj, 'b t h d -> b h t d')\n",
    "    return proj\n",
    "\n",
    "  def attention(self, key, query, value, attention_mask):\n",
    "    \"\"\"\n",
    "    TODO-1: Compute scaled dot-product attention for GPT-2.\n",
    "\n",
    "    Implementation hints:\n",
    "    1. Compute raw attention scores using QK^T, and scale them by sqrt(d_k).\n",
    "    2. Apply a causal mask (lower-triangular) to prevent attending to future tokens.\n",
    "    3. Optionally add the external attention_mask (e.g., padding positions).\n",
    "    4. Normalize the scores with softmax to obtain attention probabilities.\n",
    "    5. Apply dropout on the probabilities.\n",
    "    6. Use them to weight the values (V) and obtain the context vectors.\n",
    "    7. Finally, merge all attention heads back into a single hidden representation.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #query=(num_queries,d_k)\n",
    "    #compute raw attention scores\n",
    "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.attention_head_size)\n",
    "    #apply causal mask\n",
    "    #create the lower triangular matrix\n",
    "    seq_len = attention_scores.size(-1)\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=attention_scores.device))\n",
    "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0) #add batch and head dimensions\n",
    "    #apply mask, set positions with 0 to -inf\n",
    "    attention_scores = attention_scores.masked_fill(causal_mask == 0, -1e9)\n",
    "    #add external attention mask\n",
    "    if attention_mask is not None:\n",
    "      attention_scores = attention_scores + attention_mask\n",
    "    #normalize scores with softmax\n",
    "    attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "    #apply dropout\n",
    "    attention_probs = self.dropout(attention_probs)\n",
    "    #weigh the values(attention_prob@value)\n",
    "    context_vector=torch.matmul(attention_probs,value)\n",
    "    #merge all attention heads into 1\n",
    "    batch_size, num_heads, seq_len, head_size = context_vector.size()\n",
    "    context = context_vector.transpose(1, 2).contiguous().reshape(batch_size, seq_len, num_heads * head_size)\n",
    "\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "  def forward(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    hidden_states: [bs, seq_len, hidden_state]\n",
    "    attention_mask: [bs, 1, 1, seq_len]\n",
    "    output: [bs, seq_len, hidden_state]\n",
    "    \"\"\"\n",
    "    # First, we have to generate the key, value, query for each token for multi-head attention\n",
    "    # using self.transform (more details inside the function).\n",
    "    # Size of *_layer is [bs, num_attention_heads, seq_len, attention_head_size].\n",
    "    key_layer = self.transform(hidden_states, self.key)\n",
    "    value_layer = self.transform(hidden_states, self.value)\n",
    "    query_layer = self.transform(hidden_states, self.query)\n",
    "    \n",
    "    # Calculate the multi-head attention using the self.attention function.\n",
    "    attn_value = self.attention(key_layer, query_layer, value_layer, attention_mask)\n",
    "    return attn_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc9ad59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:22:28.327719Z",
     "iopub.status.busy": "2025-12-06T10:22:28.327488Z",
     "iopub.status.idle": "2025-12-06T10:22:28.333653Z",
     "shell.execute_reply": "2025-12-06T10:22:28.332975Z",
     "shell.execute_reply.started": "2025-12-06T10:22:28.327703Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPT2Layer(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    # Multi-head attention.\n",
    "    self.self_attention = CausalSelfAttention(config)\n",
    "    # Add-norm for multi-head attention.\n",
    "    self.attention_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.attention_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "    self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    # Feed forward.\n",
    "    self.interm_dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "    self.interm_af = F.gelu\n",
    "    # Add-norm for feed forward.\n",
    "    self.out_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    self.out_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "    self.out_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "  def add(self, input, output, dense_layer, dropout):\n",
    "    \"\"\"\n",
    "    TODO-2: Residual connection with dense projection and dropout.\n",
    "    \n",
    "    Implementation hints:\n",
    "    1. Project the 'output' through dense_layer.\n",
    "    2. Apply dropout to prevent overfitting.\n",
    "    3. Add the original 'input' (residual connection) to the processed output.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #project output through dense_layer\n",
    "    projected=dense_layer(output)\n",
    "    #apply dropout\n",
    "    projected=dropout(projected)\n",
    "    #add OG input to output\n",
    "    return input+projected\n",
    "\n",
    "\n",
    "  def forward(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    TODO-3: Forward pass of a GPT-2 layer.\n",
    "\n",
    "    Implementation hints:\n",
    "    ---- Self-Attention Block ----\n",
    "    1. LayerNorm the input for stability using self.attention_layer_norm.\n",
    "    2. Compute multi-head causal self-attention using self.self_attention.\n",
    "    3. Apply residual connection using self.add (dense_layer=self.attention_dense, dropout=self.attention_dropout).\n",
    "\n",
    "    ---- Feed Forward Block ----\n",
    "    4. LayerNorm the hidden_states from attention block using self.out_layer_norm.\n",
    "    5. Pass through a two-layer feed-forward network with activation:\n",
    "       self.interm_dense -> self.interm_af -> self.out_dense\n",
    "    6. Apply residual connection again using self.add (dense_layer=self.out_dense, dropout=self.out_dropout).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #1. layernorm the input\n",
    "    normed_input=self.attention_layer_norm(hidden_states)\n",
    "    #2. compute the self attention\n",
    "    attn_output=self.self_attention(normed_input,attention_mask)\n",
    "    #3. apply residual connection\n",
    "    hidden_states=self.add(hidden_states,attn_output,self.attention_dense,self.attention_dropout)\n",
    "    #4. layernorm the hidden states from attention blocks\n",
    "    normed_ff=self.out_layer_norm(hidden_states)\n",
    "    #5. pass through 2 layer feed forward\n",
    "    ff_output=self.interm_dense(normed_ff)\n",
    "    ff_output=self.interm_af(ff_output)\n",
    "    ff_output = self.out_dense(ff_output)\n",
    "    ff_output = self.out_dropout(ff_output)\n",
    "    #6. apply residual connection\n",
    "    hidden_states = hidden_states + ff_output\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a04146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:21:17.596785Z",
     "iopub.status.busy": "2025-12-06T10:21:17.596146Z",
     "iopub.status.idle": "2025-12-06T10:21:17.601790Z",
     "shell.execute_reply": "2025-12-06T10:21:17.601163Z",
     "shell.execute_reply.started": "2025-12-06T10:21:17.596751Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPTPreTrainedModel(nn.Module):\n",
    "\n",
    "  def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    self.name_or_path = config.name_or_path\n",
    "\n",
    "  def init_weights(self):\n",
    "    # Initialize weights\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "    \"\"\" Initialize the weights \"\"\"\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "      # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "      # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "      module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "      module.bias.data.zero_()\n",
    "      module.weight.data.fill_(1.0)\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "      module.bias.data.zero_()\n",
    "\n",
    "  @property\n",
    "  def dtype(self) -> dtype:\n",
    "    return get_parameter_dtype(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68d1cd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:21:20.413913Z",
     "iopub.status.busy": "2025-12-06T10:21:20.413574Z",
     "iopub.status.idle": "2025-12-06T10:21:20.431846Z",
     "shell.execute_reply": "2025-12-06T10:21:20.431170Z",
     "shell.execute_reply.started": "2025-12-06T10:21:20.413887Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPT2Model(GPTPreTrainedModel):\n",
    "  \"\"\"\n",
    "  The GPT model returns the final embeddings for each token in a sentence.\n",
    "\n",
    "  The model consists of:\n",
    "  1. Embedding layers (used in self.embed).\n",
    "  2. A stack of n GPT layers (used in self.encode).\n",
    "  3. A linear transformation layer for the [CLS] token (used in self.forward, as given).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "    self.config = config\n",
    "\n",
    "    # Embedding layers.\n",
    "    self.word_embedding = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "    self.pos_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "    self.embed_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # Register position_ids (1, len position emb) to buffer because it is a constant.\n",
    "    position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)\n",
    "    self.register_buffer('position_ids', position_ids)\n",
    "\n",
    "    # GPT-2 layers.\n",
    "    self.gpt_layers = nn.ModuleList([GPT2Layer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    # [CLS] token transformations.\n",
    "    self.pooler_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.pooler_af = nn.Tanh()\n",
    "\n",
    "    # Final layer norm.\n",
    "    self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def embed(self, input_ids):\n",
    "    \"\"\"\n",
    "    TODO-4: Embedding layer of the GPT-2 model.\n",
    "\n",
    "    Implementation hints:\n",
    "    1. Use self.word_embedding to convert input_ids to embeddings.\n",
    "    2. Generate position ids and convert to embeddings using self.pos_embedding.\n",
    "    3. Sum token and position embeddings.\n",
    "    4. Apply self.embed_dropout to the sum.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #get token embeddings\n",
    "    token_embeddings=self.word_embedding(input_ids)\n",
    "    batch_size, seq_len = input_ids.size()\n",
    "    #generate position ids and convert to embeddings\n",
    "    position_ids = self.position_ids[:, :seq_len]\n",
    "    position_ids = position_ids.expand(batch_size, seq_len)\n",
    "    position_embeddings = self.pos_embedding(position_ids)\n",
    "    #sum token and position embeddings\n",
    "    sum_of_embeddings=token_embeddings+position_embeddings\n",
    "    sum_of_embeddings=self.embed_dropout(sum_of_embeddings)\n",
    "    return sum_of_embeddings\n",
    "\n",
    "  def encode(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    hidden_states: the output from the embedding layer [batch_size, seq_len, hidden_size]\n",
    "    attention_mask: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    # Get the extended attention mask for self-attention.\n",
    "    # Returns extended_attention_mask of size [batch_size, 1, 1, seq_len].\n",
    "    # Distinguishes between non-padding tokens (with a value of 0) and padding tokens\n",
    "    # (with a value of a large negative number).\n",
    "    extended_attention_mask: torch.Tensor = get_extended_attention_mask(attention_mask, self.dtype)\n",
    "\n",
    "    # Pass the hidden states through the encoder layers.\n",
    "    for i, layer_module in enumerate(self.gpt_layers):\n",
    "      # Feed the encoding from the last bert_layer to the next.\n",
    "      hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    input_ids: [batch_size, seq_len], seq_len is the max length of the batch\n",
    "    attention_mask: same size as input_ids, 1 represents non-padding tokens, 0 represents padding tokens\n",
    "    \"\"\"\n",
    "    # Get the embedding for each input token.\n",
    "    embedding_output = self.embed(input_ids=input_ids)\n",
    "\n",
    "    # Feed to a transformer (a stack of GPTLayers).\n",
    "    sequence_output = self.encode(embedding_output, attention_mask=attention_mask)\n",
    "    sequence_output = self.final_layer_norm(sequence_output)\n",
    "\n",
    "    # Get the hidden state of the final token.\n",
    "    last_non_pad_idx = attention_mask.sum(dim=1) - 1  # Subtract 1 to get last index\n",
    "    last_token = sequence_output[torch.arange(sequence_output.shape[0]), last_non_pad_idx]\n",
    "\n",
    "    return {'last_hidden_state': sequence_output, 'last_token': last_token}\n",
    "\n",
    "  def hidden_state_to_token(self, hidden_state):\n",
    "    \"\"\"\n",
    "    TODO-5: Convert hidden states back to token logits.\n",
    "\n",
    "    Implementation hints: \n",
    "    - GPT-2 uses weight tying with the input word embeddings. \n",
    "    - The logits are the dot product between output hidden states and the word embedding weights: hidden_state(s) * E^T\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    logits = torch.matmul(hidden_state, self.word_embedding.weight.T)\n",
    "    return logits\n",
    "\n",
    "  @classmethod\n",
    "  def from_pretrained(cls, model='gpt2', d=768, l=12, num_heads=12):\n",
    "    gpt_model = OpenAIGPT2Model.from_pretrained(model).eval()\n",
    "    our_model = GPT2Model(GPT2Config(hidden_size=d, num_hidden_layers=l,num_attention_heads=num_heads,\n",
    "                                     intermediate_size=d*3)).eval()\n",
    "\n",
    "    # Load word and positional embeddings.\n",
    "    our_model.word_embedding.load_state_dict(gpt_model.wte.state_dict())\n",
    "    our_model.pos_embedding.load_state_dict(gpt_model.wpe.state_dict())\n",
    "\n",
    "    for i in range(l):\n",
    "      l = our_model.gpt_layers[i]\n",
    "      # Remap the Q,K,V weights from a conv1d to 3 linear projections\n",
    "      l.self_attention.query.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, :d].T\n",
    "      l.self_attention.query.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][:d]\n",
    "      l.self_attention.key.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d:d*2].T\n",
    "      l.self_attention.key.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d:d*2]\n",
    "      l.self_attention.value.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d*2:].T\n",
    "      l.self_attention.value.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d*2:]\n",
    "\n",
    "      # Remap final dense layer in MHA.\n",
    "      l.attention_dense.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.weight'].T\n",
    "      l.attention_dense.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.bias']\n",
    "\n",
    "      # Remap attention layer norm.\n",
    "      l.attention_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_1.weight']\n",
    "      l.attention_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_1.bias']\n",
    "\n",
    "      # Remap post-attention MLP layers.\n",
    "      l.interm_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.weight'].T\n",
    "      l.interm_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.bias']\n",
    "      l.out_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.weight'].T\n",
    "      l.out_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.bias']\n",
    "\n",
    "      # Remap second layer norm weights.\n",
    "      l.out_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_2.weight']\n",
    "      l.out_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_2.bias']\n",
    "\n",
    "    # Remap the final layer norm values.\n",
    "    our_model.final_layer_norm.weight.data = gpt_model.state_dict()['ln_f.weight']\n",
    "    our_model.final_layer_norm.bias.data = gpt_model.state_dict()['ln_f.bias']\n",
    "\n",
    "    return our_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c95668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T03:06:32.371620Z",
     "iopub.status.busy": "2025-12-05T03:06:32.370841Z",
     "iopub.status.idle": "2025-12-05T03:06:39.220103Z",
     "shell.execute_reply": "2025-12-05T03:06:39.219502Z",
     "shell.execute_reply.started": "2025-12-05T03:06:32.371596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1590b2ebbca41ebb63e2045c4b95d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your GPT2 implementation is correct!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: compare with Huggingface GPT2 implementation\n",
    "def test_gpt2(model_size='gpt2'):\n",
    "  sent_ids = torch.tensor([[101, 7592, 2088, 102, 0, 0, 0, 0],\n",
    "                           [101, 7592, 15756, 2897, 2005, 17953, 2361, 102]])\n",
    "  att_mask = torch.tensor([[1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "  # Load both the OpenAI and your own model.\n",
    "  openai_model = OpenAIGPT2Model.from_pretrained(model_size)\n",
    "  gpt = GPT2Model.from_pretrained(model=model_size, **model_size_to_params(model_size))\n",
    "\n",
    "  outputs = gpt(sent_ids, att_mask)\n",
    "  openai_outputs = openai_model(input_ids=sent_ids, attention_mask=att_mask, output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "  att_mask = att_mask.unsqueeze(-1)\n",
    "  outputs['last_hidden_state'] = outputs['last_hidden_state'] * att_mask\n",
    "  openai_outputs *= att_mask\n",
    "\n",
    "  assert torch.allclose(outputs['last_hidden_state'], openai_outputs, atol=1e-1, rtol=1e-2)\n",
    "\n",
    "  print(\"Your GPT2 implementation is correct!\")\n",
    "\n",
    "test_gpt2('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a449a7",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35494be8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T09:57:55.380100Z",
     "iopub.status.busy": "2025-12-06T09:57:55.379199Z",
     "iopub.status.idle": "2025-12-06T09:57:55.390144Z",
     "shell.execute_reply": "2025-12-06T09:57:55.389687Z",
     "shell.execute_reply.started": "2025-12-06T09:57:55.380066Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            params: Iterable[torch.nn.parameter.Parameter],\n",
    "            lr: float = 1e-3,\n",
    "            betas: Tuple[float, float] = (0.9, 0.999),\n",
    "            eps: float = 1e-6,\n",
    "            weight_decay: float = 0.0,\n",
    "            correct_bias: bool = True,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Callable = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                # State should be stored in this dictionary.\n",
    "                state = self.state[p]\n",
    "\n",
    "                # Access hyperparameters from the `group` dictionary.\n",
    "                lr = group[\"lr\"]\n",
    "                eps = group[\"eps\"]\n",
    "                weight_decay = group[\"weight_decay\"]\n",
    "                correct_bias = group[\"correct_bias\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                \n",
    "                state[\"step\"] += 1\n",
    "                t = state[\"step\"]\n",
    "                \n",
    "                \"\"\"\n",
    "                TODO-6: Implement the AdamW parameter update for this step.\n",
    "\n",
    "                Implementation hints:\n",
    "                1. Update biased first moment estimate:\n",
    "                    m_t = beta1 * m_{t-1} + (1 - beta1) * grad\n",
    "                2. Update biased second raw moment estimate:\n",
    "                    v_t = beta2 * v_{t-1} + (1 - beta2) * grad^2\n",
    "                3. Apply bias correction if correct_bias=True:\n",
    "                    m_hat = m_t / (1 - beta1^t)\n",
    "                    v_hat = v_t / (1 - beta2^t)\n",
    "                4. Compute step size:\n",
    "                    step_size = lr (or lr / (1 - beta1^t) if bias correction)\n",
    "                5. Update parameters:\n",
    "                    p = p - step_size * m_hat / (sqrt(v_hat) + eps)\n",
    "                6. Apply decoupled weight decay after the parameter update (if weight_decay > 0):\n",
    "                    p = p - lr * weight_decay * p\n",
    "                Reference:\n",
    "                Algorithm 1 in \"Adam: A Method for Stochastic Optimization\"\n",
    "                https://arxiv.org/abs/1412.6980\n",
    "                \"\"\"\n",
    "                ### YOUR CODE HERE\n",
    "                exp_avg = state[\"exp_avg\"]\n",
    "                exp_avg_sq = state[\"exp_avg_sq\"]\n",
    "\n",
    "                # update biased first and second moments\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # bias correction\n",
    "                if correct_bias:\n",
    "                    m_hat = exp_avg / (1 - beta1 ** t)\n",
    "                    v_hat = exp_avg_sq / (1 - beta2 ** t)\n",
    "                else:\n",
    "                    m_hat = exp_avg\n",
    "                    v_hat = exp_avg_sq\n",
    "\n",
    "                # update p\n",
    "                p.data.addcdiv_(-lr, m_hat, torch.sqrt(v_hat) + eps)\n",
    "\n",
    "                # apply weight decay\n",
    "                if weight_decay > 0:\n",
    "                    p.data.add_(-lr * weight_decay, p.data)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "544e2a8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T03:06:39.330256Z",
     "iopub.status.busy": "2025-12-05T03:06:39.329650Z",
     "iopub.status.idle": "2025-12-05T03:06:39.705808Z",
     "shell.execute_reply": "2025-12-05T03:06:39.705360Z",
     "shell.execute_reply.started": "2025-12-05T03:06:39.330237Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40/3373583738.py:93: UserWarning: This overload of addcdiv_ is deprecated:\n",
      "\taddcdiv_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcdiv_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  p.data.addcdiv_(-lr, m_hat, torch.sqrt(v_hat) + eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5548,  0.8667,  0.0729],\n",
      "        [-0.4472, -0.2951, -0.2717]])\n",
      "tensor([[ 0.5548,  0.8667,  0.0729],\n",
      "        [-0.4472, -0.2951, -0.2717]])\n",
      "Optimizer test passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for AdamW optimizer\n",
    "def test_optimizer(opt_class) -> torch.Tensor:\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    model = torch.nn.Linear(3, 2, bias=False)\n",
    "    opt = opt_class(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        correct_bias=True,\n",
    "    )\n",
    "    for i in range(1000):\n",
    "        opt.zero_grad()\n",
    "        x = torch.FloatTensor(rng.uniform(size=[model.in_features]))\n",
    "        y_hat = model(x)\n",
    "        y = torch.Tensor([x[0] + x[1], -x[2]])\n",
    "        loss = ((y - y_hat) ** 2).sum()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return model.weight.detach()\n",
    "\n",
    "SEED = 0\n",
    "ref = torch.tensor(np.load(\"optimizer_test.npy\"))\n",
    "actual = test_optimizer(AdamW)\n",
    "print(ref)\n",
    "print(actual)\n",
    "assert torch.allclose(ref, actual, atol=1e-6, rtol=1e-4)\n",
    "print(\"Optimizer test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249d1f0",
   "metadata": {},
   "source": [
    "## Model Loading & Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc299457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:21:31.629001Z",
     "iopub.status.busy": "2025-12-06T10:21:31.628301Z",
     "iopub.status.idle": "2025-12-06T10:21:31.633903Z",
     "shell.execute_reply": "2025-12-06T10:21:31.633394Z",
     "shell.execute_reply.started": "2025-12-06T10:21:31.628927Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_gpt2(model, tokenizer, input_ids, max_gen_length=50, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate text from a GPT-2 model given a single input sequence (greedy decoding).\n",
    "\n",
    "    Note:\n",
    "        - Currently only supports batch_size=1 (single input sequence).\n",
    "        - Using greedy decoding, so each run with the same input produces the same output.\n",
    "        - Other sampling-based decoding methods (e.g., top-k, top-p, temperature) can introduce randomness and yield different outputs each run.\n",
    "\n",
    "    Args:\n",
    "        model: GPT-2 model (pretrained or fine-tuned)\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        input_ids: torch.LongTensor of shape [1, seq_len], input token IDs\n",
    "        max_gen_length: int, maximum number of tokens to generate\n",
    "        device: str, \"cuda\" or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)  # move input to device\n",
    "    output_ids = input_ids.clone()\n",
    "\n",
    "    \"\"\"\n",
    "    TODO-8: Greedy next-token generation loop\n",
    "\n",
    "    Implementation hints:\n",
    "    Repeat the below steps up to max_gen_length:\n",
    "    1. Construct an attention mask based on current output_ids (non-pad tokens).\n",
    "    2. Pass output_ids and attention_mask through the model to get hidden states.\n",
    "    3. Convert the last hidden state to logits over the vocabulary using model.hidden_state_to_token.\n",
    "    4. Select the next token using greedy decoding (argmax over logits).\n",
    "    5. Append the next token to output_ids.\n",
    "    6. Stop the loop early if the EOS token is generated.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    for _ in range(max_gen_length):\n",
    "        attention_mask=torch.ones_like(output_ids)\n",
    "        hidden_states=model(output_ids,attention_mask) #hidden state is a dictionary w keys last_hidden_state and last_token\n",
    "        # print(type(hidden_states))\n",
    "        # print(hidden_states.keys() if hasattr(hidden_states, 'keys') else hidden_states)\n",
    "        last_hidden = hidden_states['last_hidden_state'][:, -1, :] #shape : [batch_size, sequence_length, hidden_dim]\n",
    "        logits = model.hidden_state_to_token(last_hidden)\n",
    "        next_token = torch.argmax(logits,dim=-1).unsqueeze(-1)\n",
    "        output_ids = torch.cat([output_ids, next_token],dim=-1)\n",
    "        if (next_token == tokenizer.eos_token_id).any():\n",
    "            break\n",
    "\n",
    "    # raise NotImplementedError\n",
    "\n",
    "    # Decode generated tokens to string\n",
    "    ids = output_ids[0]\n",
    "    text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e7cc58d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T03:08:12.793105Z",
     "iopub.status.busy": "2025-12-05T03:08:12.792675Z",
     "iopub.status.idle": "2025-12-05T03:08:15.346652Z",
     "shell.execute_reply": "2025-12-05T03:08:15.346186Z",
     "shell.execute_reply.started": "2025-12-05T03:08:12.793084Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load a pretrained GPT-2 model with official weights\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = GPT2Model.from_pretrained(\"gpt2\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b611373",
   "metadata": {},
   "source": [
    "## Load NLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f25161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:21:34.702322Z",
     "iopub.status.busy": "2025-12-06T10:21:34.702084Z",
     "iopub.status.idle": "2025-12-06T10:21:34.723869Z",
     "shell.execute_reply": "2025-12-06T10:21:34.723409Z",
     "shell.execute_reply.started": "2025-12-06T10:21:34.702302Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "    correct = sum(p.lower().strip() == l.lower().strip() for p, l in zip(preds, labels))\n",
    "    return correct / len(labels)\n",
    "\n",
    "def evaluate_gpt2_xnli(model, tokenizer, dataloader, max_gen_length=10, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader, desc=\"Generating\"):\n",
    "            input_ids = item['input_ids']\n",
    "            gen_text = generate_gpt2(model, tokenizer, input_ids, max_gen_length=max_gen_length, device=device)\n",
    "            pred_label = gen_text.split(\"Label:\")[-1].strip()\n",
    "            all_preds.append(pred_label)\n",
    "            all_labels.extend(item['label_strs'])\n",
    "    acc = compute_accuracy(all_preds, all_labels)\n",
    "    print(f\"Evaluation accuracy: {acc*100:.2f}%\")\n",
    "    return acc, all_preds, all_labels\n",
    "\n",
    "class XNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for XNLI (Cross-lingual Natural Language Inference) task.\n",
    "\n",
    "    Supports train, dev, and test splits in a specific language, \n",
    "    tokenizes text inputs for GPT-style models, and optionally subsamples the dataset.\n",
    "\n",
    "    Attributes:\n",
    "        split (str): Dataset split, one of 'train', 'dev', 'test'.\n",
    "        lang (str): Language code (e.g., 'en', 'zh').\n",
    "        tokenizer: A HuggingFace tokenizer to convert text to input IDs.\n",
    "        max_length (int): Maximum sequence length for tokenization.\n",
    "        LABEL2ID (dict): Mapping from textual labels to integer IDs.\n",
    "        ID2LABEL (dict): Reverse mapping from integer IDs to textual labels.\n",
    "        data (pd.DataFrame): The loaded and preprocessed dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        split=\"train\",\n",
    "        lang=\"en\",\n",
    "        train_path_template=\"XNLI-MT-1.0/multinli/multinli.train.{lang}.tsv\",\n",
    "        test_path=\"XNLI-1.0/xnli.test.tsv\",\n",
    "        dev_path=\"XNLI-1.0/xnli.dev.tsv\",\n",
    "        tokenizer=None,\n",
    "        max_length=1024,\n",
    "        subset = 1.0  # 0~1\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.lang = lang\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.LABEL2ID = {\"entailment\": 0, \"contradictory\": 1, \"neutral\": 2}\n",
    "        self.ID2LABEL = {v: k for k, v in self.LABEL2ID.items()}\n",
    "\n",
    "        if split == \"train\":\n",
    "            path = train_path_template.format(lang=lang)\n",
    "            df = self.read_xnli_tsv(path, split)\n",
    "            df = df.dropna(subset=['premise','hypo','label'])\n",
    "        elif split in [\"dev\", \"test\"]:\n",
    "            path = test_path if split==\"test\" else dev_path\n",
    "            df = self.read_xnli_tsv(path, split)\n",
    "            df = df[df['language']==lang].copy()\n",
    "            keep_cols = ['sentence1', 'sentence2', 'gold_label']\n",
    "            df = df[keep_cols].dropna()\n",
    "            df.rename(columns={'sentence1':'premise','sentence2':'hypo','gold_label':'label'}, inplace=True)\n",
    "            df['label'] = df['label'].replace({'contradiction': 'contradictory'})\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of ['train','dev','test']\")\n",
    "        \n",
    "        original_num = len(df)\n",
    "        if subset < 1.0:\n",
    "            n = max(1, int(len(df) * subset))\n",
    "            df = df.iloc[:n].reset_index(drop=True)\n",
    "        subset_num = len(df)\n",
    "\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        print(f\"Dataset initialized: split='{split}', lang='{lang}', total={original_num}, subset={subset}, subset_count={subset_num}\")\n",
    "\n",
    "    def read_xnli_tsv(self, path, split):\n",
    "        \"\"\"\n",
    "        Read an XNLI TSV file and return it as a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to the TSV file.\n",
    "            split (str): One of \"train\", \"dev\", \"test\" indicating the dataset split.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The dataset as a DataFrame with appropriate columns.\n",
    "        \"\"\"\n",
    "        if split == \"train\":\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines()\n",
    "            header = lines[0].split(\"\\t\")\n",
    "            data = []\n",
    "            for i, line in enumerate(lines[1:], start=2):\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == len(header):\n",
    "                    data.append(parts)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(parts)} cols → {parts[:2]}\")\n",
    "        else:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.reader(f, delimiter=\"\\t\")\n",
    "                rows = list(reader)\n",
    "            header = rows[0]\n",
    "            expected_cols = len(header)\n",
    "            data = []\n",
    "            for i, row in enumerate(rows[1:], start=2):\n",
    "                if len(row) == expected_cols:\n",
    "                    data.append(row)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(row)} cols → {row[:2]}\")\n",
    "        return pd.DataFrame(data, columns=header)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of examples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single example by index and tokenize it.\n",
    "\n",
    "        For training split:\n",
    "            - Constructs the input as \"Premise: ... Hypothesis: ... Label: ...\"\n",
    "            - Tokenizes the full input.\n",
    "            - Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "\n",
    "        For dev/test split:\n",
    "            - Constructs the input without label as \"Premise: ... Hypothesis: ... Label:\"\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains 'input_ids', 'attention_mask', 'labels' (train only), 'label_str'\n",
    "        \"\"\"\n",
    "        row = self.data.iloc[idx]\n",
    "        premise = row['premise']\n",
    "        hypo = row['hypo']\n",
    "        label = row['label']\n",
    "        if self.lang == 'zh': # de-tokenize for Chinese\n",
    "            premise = premise.replace(\" \", \"\")\n",
    "            hypo = hypo.replace(\" \", \"\")\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            prefix = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            full_text = prefix + str(self.LABEL2ID[label])\n",
    "            tokenized = self.tokenizer(\n",
    "                full_text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "            prefix_ids = self.tokenizer(prefix).input_ids\n",
    "            labels_ids = tokenized['input_ids'].clone()\n",
    "            labels_ids[:len(prefix_ids)] = -100 # Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "            tokenized['labels'] = labels_ids\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "        else:\n",
    "            text = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Collate a batch of examples into padded tensors.\n",
    "\n",
    "        Pads 'input_ids' and 'attention_mask' to the max length in the batch.\n",
    "        Pads 'labels' with -100 if present.\n",
    "        Collects 'label_str' for reference.\n",
    "\n",
    "        Returns:\n",
    "            dict: Padded tensors and label strings for the batch.\n",
    "        \"\"\"\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            [b['input_ids'] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            [b['attention_mask'] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "\n",
    "        if 'labels' in batch[0]:\n",
    "            labels = torch.nn.utils.rnn.pad_sequence(\n",
    "                [b['labels'] for b in batch],\n",
    "                batch_first=True,\n",
    "                padding_value=-100\n",
    "            )\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        label_strs = [b['label_str'] for b in batch]\n",
    "\n",
    "        out = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label_strs\": label_strs}\n",
    "        if labels is not None:\n",
    "            out[\"labels\"] = labels\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57fdf1",
   "metadata": {},
   "source": [
    "## Zero-shot Cross-lingual Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a247f1d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T06:37:32.238782Z",
     "iopub.status.busy": "2025-12-06T06:37:32.238561Z",
     "iopub.status.idle": "2025-12-06T06:37:32.241863Z",
     "shell.execute_reply": "2025-12-06T06:37:32.241281Z",
     "shell.execute_reply.started": "2025-12-06T06:37:32.238766Z"
    }
   },
   "outputs": [],
   "source": [
    "langs = ['en', 'ar', 'bg', 'de','el','es','fr','hi','ru','sw','th','tr','ur','vi','zh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1f58b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T06:38:54.686238Z",
     "iopub.status.busy": "2025-12-06T06:38:54.685990Z",
     "iopub.status.idle": "2025-12-06T06:52:27.778729Z",
     "shell.execute_reply": "2025-12-06T06:52:27.778117Z",
     "shell.execute_reply.started": "2025-12-06T06:38:54.686221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='test', lang='en', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='ar', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='bg', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='de', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='el', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='es', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='fr', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='hi', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='ru', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='sw', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='th', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='tr', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='ur', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='vi', total=5010, subset=1, subset_count=5010\n",
      "Dataset initialized: split='test', lang='zh', total=5010, subset=1, subset_count=5010\n",
      "Evaluating on en...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:47<00:00, 106.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 83.23%\n",
      "Evaluating zero-shot on ar...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:53<00:00, 92.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 37.19%\n",
      "Evaluating zero-shot on bg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:55<00:00, 90.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 37.72%\n",
      "Evaluating zero-shot on de...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:49<00:00, 101.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 40.28%\n",
      "Evaluating zero-shot on el...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:57<00:00, 86.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 35.79%\n",
      "Evaluating zero-shot on es...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:48<00:00, 104.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 46.47%\n",
      "Evaluating zero-shot on fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:49<00:00, 102.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 43.83%\n",
      "Evaluating zero-shot on hi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:59<00:00, 83.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 34.55%\n",
      "Evaluating zero-shot on ru...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:56<00:00, 88.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 37.52%\n",
      "Evaluating zero-shot on sw...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:49<00:00, 102.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 39.92%\n",
      "Evaluating zero-shot on th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [01:02<00:00, 80.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 34.85%\n",
      "Evaluating zero-shot on tr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:50<00:00, 99.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 40.50%\n",
      "Evaluating zero-shot on ur...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:55<00:00, 90.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 34.93%\n",
      "Evaluating zero-shot on vi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:53<00:00, 93.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 39.22%\n",
      "Evaluating zero-shot on zh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:51<00:00, 96.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 37.94%\n",
      "Zero-shot cross-lingual accuracy per language:\n",
      "en: 83.23%\n",
      "ar: 37.19%\n",
      "bg: 37.72%\n",
      "de: 40.28%\n",
      "el: 35.79%\n",
      "es: 46.47%\n",
      "fr: 43.83%\n",
      "hi: 34.55%\n",
      "ru: 37.52%\n",
      "sw: 39.92%\n",
      "th: 34.85%\n",
      "tr: 40.50%\n",
      "ur: 34.93%\n",
      "vi: 39.22%\n",
      "zh: 37.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SUBSET = 1\n",
    "path = \"./best_model/model_en_epoch_3.pt\" # To-do: Swap this naming convension\n",
    "\n",
    "finetuned_model = GPT2Model(GPT2Config()).to(DEVICE)\n",
    "finetuned_model.load_state_dict(torch.load(path))\n",
    "\n",
    "all_test_datasets = {}\n",
    "all_test_loader = {}\n",
    "for lang in langs:\n",
    "    test_dataset = XNLIDataset(split=\"test\", lang=lang, tokenizer=tokenizer, max_length=1024, subset=TEST_SUBSET)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=XNLIDataset.collate_fn)\n",
    "    all_test_datasets[lang] = test_dataset\n",
    "    all_test_loader[lang] = test_loader\n",
    "\n",
    "all_results = {}\n",
    "for lang in langs:\n",
    "    test_loader = all_test_loader[lang]\n",
    "    if lang == \"en\":\n",
    "        print(f\"Evaluating on {lang}...\")\n",
    "    else:\n",
    "        print(f\"Evaluating zero-shot on {lang}...\")\n",
    "    acc, all_preds, all_labels = evaluate_gpt2_xnli(finetuned_model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
    "    all_results[lang] = acc\n",
    "\n",
    "print(\"Zero-shot cross-lingual accuracy per language:\")\n",
    "for lang, acc in all_results.items():\n",
    "    print(f\"{lang}: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16af93d-7b8b-439f-9100-f91d21d1ff17",
   "metadata": {},
   "source": [
    "### Extra: Comparing fine-tuned model from Task 2 with Chat-GPT's default weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64a5d31d-7ed3-4a3c-80f6-281acca45d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T08:05:20.706716Z",
     "iopub.status.busy": "2025-11-20T08:05:20.706401Z",
     "iopub.status.idle": "2025-11-20T08:05:20.713749Z",
     "shell.execute_reply": "2025-11-20T08:05:20.712840Z",
     "shell.execute_reply.started": "2025-11-20T08:05:20.706690Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_label(model, tokenizer, input_ids, attention_mask, max_gen_length=1):\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_gen_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False  # Greedy decoding\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                            \n",
    "                            \n",
    "\n",
    "def evaluate_official_gpt2_xnli(model, tokenizer, dataloader, max_gen_length=1):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        label_strs = batch[\"label_strs\"]\n",
    "\n",
    "        gen_text = generate_label(model, tokenizer, input_ids, attention_mask, max_gen_length)\n",
    "        pred_label = gen_text.split(\"Label:\")[-1].strip()  # Extract after \"Label:\"\n",
    "        all_preds.append(pred_label)\n",
    "        all_labels.extend(label_strs)\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = sum(p == l for p, l in zip(all_preds, all_labels))\n",
    "    acc = correct / len(all_labels)\n",
    "    return acc, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a2aed9f-e5a3-4962-a07f-bf84357aa9a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T08:05:23.719068Z",
     "iopub.status.busy": "2025-11-20T08:05:23.718780Z",
     "iopub.status.idle": "2025-11-20T08:30:53.989836Z",
     "shell.execute_reply": "2025-11-20T08:30:53.989169Z",
     "shell.execute_reply.started": "2025-11-20T08:05:23.719046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on en...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:31<00:00, 54.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on ar...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:41<00:00, 49.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on bg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:47<00:00, 46.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on de...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:34<00:00, 52.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on el...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:48<00:00, 46.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on es...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:30<00:00, 55.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:34<00:00, 53.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on hi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:53<00:00, 44.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on ru...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:48<00:00, 46.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on sw...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:31<00:00, 54.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [02:01<00:00, 41.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on tr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:34<00:00, 52.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on ur...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:50<00:00, 45.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on vi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:42<00:00, 48.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating zero-shot on zh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5010/5010 [01:35<00:00, 52.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-shot cross-lingual accuracy:\n",
      "en: 0.02%\n",
      "ar: 0.00%\n",
      "bg: 0.00%\n",
      "de: 0.00%\n",
      "el: 0.00%\n",
      "es: 0.02%\n",
      "fr: 0.02%\n",
      "hi: 0.00%\n",
      "ru: 0.00%\n",
      "sw: 0.00%\n",
      "th: 0.00%\n",
      "tr: 0.02%\n",
      "ur: 0.02%\n",
      "vi: 0.04%\n",
      "zh: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS\n",
    "\n",
    "official_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE).eval()\n",
    "\n",
    "\n",
    "all_results = {}\n",
    "for lang in langs:\n",
    "    print(f\"Evaluating zero-shot on {lang}...\")\n",
    "    acc, preds, labels = evaluate_official_gpt2_xnli(model, tokenizer, all_test_loader[lang])\n",
    "    all_results[lang] = acc\n",
    "\n",
    "print(\"\\nZero-shot cross-lingual accuracy:\")\n",
    "for lang, acc in all_results.items():\n",
    "    print(f\"{lang}: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d87e2",
   "metadata": {},
   "source": [
    "## Fertility-based Language Selection\n",
    "\n",
    "Guidance: You may notice that some languages achieve reasonable zero-shot cross-lingual performance. This is likely because these languages are closer to English (e.g., in writing system), making cross-lingual transfer from English easier. However, many other languages perform close to random guessing, which is expected since GPT-2 was pretrained entirely on English data.\n",
    "\n",
    "To perform further multilingual fine-tuning, we need to identify which languages GPT-2 can realistically support (because if a language is not supported, fine-tuning on it will have little effect). A straightforward way to check this is to inspect the tokens in the model’s tokenizer. However, this is not practical for GPT-2-like models, because they use a Byte-Pair Encoding (BPE) tokenizer. BPE can decompose any Unicode string into subwords, even if the string never appeared in training, making it difficult to determine whether a language is truly supported.\n",
    "\n",
    "Instead, we can approximate tokenizer support using fertility, a metric that measures the average number of subwords produced per word. Lower fertility indicates better tokenizer quality and compression, while high fertility suggests heavy fragmentation, which can hurt model performance. By combining fertility analysis with zero-shot cross-lingual results, we can identify a subset of languages that GPT-2 can reasonably handle (a rough estimate, as officially GPT-2 is designed for English). Then, we can proceed with multilingual fine-tuning experiments on these languages.\n",
    "\n",
    "Reference: How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d28acaf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T02:18:01.848787Z",
     "iopub.status.busy": "2025-11-21T02:18:01.848034Z",
     "iopub.status.idle": "2025-11-21T02:18:01.852804Z",
     "shell.execute_reply": "2025-11-21T02:18:01.852127Z",
     "shell.execute_reply.started": "2025-11-21T02:18:01.848764Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_fertility(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute average fertility for a dataset.\n",
    "    Fertility = #tokens / #words\n",
    "    Note: word splitting is approximate and uses whitespace.\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    total_tokens = 0\n",
    "    samples = len(dataset)\n",
    "    \n",
    "    for i in tqdm(range(samples), desc=\"Computing fertility\"):\n",
    "        row = dataset.data.iloc[i]\n",
    "        for sent in [row['premise'], row['hypo']]:\n",
    "            words = sent.strip().split()  # crude word estimate\n",
    "            tokens = tokenizer.tokenize(sent)\n",
    "            total_words += len(words)\n",
    "            total_tokens += len(tokens)\n",
    "    \n",
    "    fertility = total_tokens / total_words if total_words > 0 else 0.0\n",
    "    return fertility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ce5b291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T12:39:58.277743Z",
     "iopub.status.busy": "2025-11-20T12:39:58.277407Z",
     "iopub.status.idle": "2025-11-20T12:40:57.434549Z",
     "shell.execute_reply": "2025-11-20T12:40:57.433573Z",
     "shell.execute_reply.started": "2025-11-20T12:39:58.277719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='train', lang='en', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:00<00:00, 4278.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: fertility = 1.11\n",
      "Dataset initialized: split='train', lang='ar', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:00<00:00, 3947.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar: fertility = 4.70\n",
      "Dataset initialized: split='train', lang='bg', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 3276.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg: fertility = 5.53\n",
      "Dataset initialized: split='train', lang='de', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 3916.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de: fertility = 2.10\n",
      "Dataset initialized: split='train', lang='el', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 3536.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el: fertility = 6.16\n",
      "Dataset initialized: split='train', lang='es', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 3802.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es: fertility = 1.83\n",
      "Dataset initialized: split='train', lang='fr', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 3611.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr: fertility = 1.75\n",
      "Dataset initialized: split='train', lang='hi', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 1974.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi: fertility = 5.12\n",
      "Dataset initialized: split='train', lang='ru', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 3435.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru: fertility = 5.90\n",
      "Dataset initialized: split='train', lang='sw', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:00<00:00, 4257.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sw: fertility = 2.08\n",
      "Dataset initialized: split='train', lang='th', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 2456.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th: fertility = 9.48\n",
      "Dataset initialized: split='train', lang='tr', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:00<00:00, 4034.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr: fertility = 2.73\n",
      "Dataset initialized: split='train', lang='ur', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 3884.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ur: fertility = 5.16\n",
      "Dataset initialized: split='train', lang='vi', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:01<00:00, 3551.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi: fertility = 3.62\n",
      "Dataset initialized: split='train', lang='zh', total=392702, subset=0.01, subset_count=3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fertility: 100%|██████████| 3927/3927 [00:00<00:00, 3954.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zh: fertility = 3.77\n",
      "\n",
      "Fertility scores:\n",
      "en: 1.11\n",
      "ar: 4.70\n",
      "bg: 5.53\n",
      "de: 2.10\n",
      "el: 6.16\n",
      "es: 1.83\n",
      "fr: 1.75\n",
      "hi: 5.12\n",
      "ru: 5.90\n",
      "sw: 2.08\n",
      "th: 9.48\n",
      "tr: 2.73\n",
      "ur: 5.16\n",
      "vi: 3.62\n",
      "zh: 3.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "subset_for_check = 0.01\n",
    "path = \"./best_model/model.pt\"\n",
    "\n",
    "finetuned_model = GPT2Model(GPT2Config()).to(DEVICE)\n",
    "finetuned_model.load_state_dict(torch.load(path))\n",
    "\n",
    "all_results = {}\n",
    "for lang in langs:\n",
    "    train_dataset = XNLIDataset(\n",
    "        split=\"train\",\n",
    "        lang=lang,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=subset_for_check\n",
    "    )\n",
    "    fertility_score = compute_fertility(train_dataset, tokenizer)\n",
    "    print(f\"{lang}: fertility = {fertility_score:.2f}\")\n",
    "    all_results[lang] = fertility_score\n",
    "    \n",
    "print(\"\\nFertility scores:\")\n",
    "for lang, fertility_score in all_results.items():\n",
    "    print(f\"{lang}: {fertility_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37090f47",
   "metadata": {},
   "source": [
    "## Fine-tune GPT-2 (per-language)\n",
    "\n",
    "Guidance: Load the pretrained GPT-2 (not the ones fine-tuned on English NLI) along with the training data for a single target language. Choose non-English languages that performed well in the zero-shot cross-lingual transfer and fertility evaluation. It depends on you how many languages to include. Fine-tune a separate model for each selected language. Afterwards, compare these per-language fine-tuned models with the zero-shot cross-lingual transfer results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97059156-6d3f-47dd-83e6-81968fe01a6d",
   "metadata": {},
   "source": [
    "### Generic retraining pipeline\n",
    "1. Load dataset from `XNLIDataset` and split it into test and train.\n",
    "2. Use DataLoader to load it up.\n",
    "3. Use Adam optimiser from the function `AdamW(Optimizer)`.\n",
    "4. Build the training loop (intitialise weights, train, get loss, gradietn descent, early stopping based on dev_dataset accuracy/results).\n",
    "5. Save the best checkpoint per langauge\n",
    "6. Evaluate via test_set\n",
    "\n",
    "Then comapre with its zero-shot counterpart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b5b93a-da5e-4233-aa3a-17c823501dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:21:58.767347Z",
     "iopub.status.busy": "2025-12-06T10:21:58.766652Z",
     "iopub.status.idle": "2025-12-06T10:21:58.887540Z",
     "shell.execute_reply": "2025-12-06T10:21:58.886932Z",
     "shell.execute_reply.started": "2025-12-06T10:21:58.767317Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Adjust from 0 to 1, but consider 0.1 for rough testing of pipeline\n",
    "TRAIN_SUBSET = 1 #0.01 for testing, 1 for actual fine-tuning\n",
    "DEV_SUBSET = 1\n",
    "TEST_SUBSET = 1\n",
    "\n",
    "# Hyperparamter of gpt2 fine-tuning\n",
    "EPOCHS = 5 # 2 for testing, 1 for actual fine-tuning\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO=0.1\n",
    "CORRECT_BIAS = True\n",
    "SAVE_DIR = \"best_model\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "def split_dataset(lang, tokenizer):\n",
    "    train_dataset = XNLIDataset(\n",
    "        split=\"train\",\n",
    "        lang=lang,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=TRAIN_SUBSET\n",
    "    )\n",
    "\n",
    "    dev_dataset = XNLIDataset(\n",
    "        split=\"dev\",\n",
    "        lang=lang,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=DEV_SUBSET\n",
    "    )\n",
    "\n",
    "    test_dataset = XNLIDataset(\n",
    "        split=\"test\",\n",
    "        lang=lang,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=TEST_SUBSET\n",
    "    )\n",
    "    \n",
    "    return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "\n",
    "def full_finetuning_pipeline(train_dataset, dev_dataset, lang):\n",
    "    # Create DataLoaders for training and validation datasets\n",
    "    train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=XNLIDataset.collate_fn)\n",
    "    dev_loader = DataLoader(dev_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, correct_bias=CORRECT_BIAS)\n",
    "    # Include early stopping\n",
    "    patience = 2\n",
    "    epochs_without_improvement = 0\n",
    "    # Track training progress\n",
    "    global_train_losses = []\n",
    "    total_train_loss = 0.0\n",
    "    total_train_steps = 0\n",
    "    print_interval = 10\n",
    "\n",
    "    # Track best dev accuracy for model saving\n",
    "    # This only works for epoch > 1 \n",
    "    best_dev_acc = 0.0\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, warmup_steps, total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        model.train()\n",
    "        # Iterate over batches\n",
    "        loop = tqdm(train_loader, desc=\"Training\")\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)        # [B, seq_len]\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch.get(\"labels\").to(DEVICE)                    # [B, seq_len]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            hidden_states = model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']  # [B, seq_len, hidden]\n",
    "\n",
    "            \"\"\"\n",
    "            Reused from to-do 9.\n",
    "\n",
    "            Implementation hints:\n",
    "            1. Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
    "            2. Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
    "            3. Compute the cross-entropy loss, making sure positions with label=-100 are ignored.\n",
    "            4. Backpropagate and update model parameters.\n",
    "            \"\"\"\n",
    "\n",
    "            ### YOUR CODE HERE\n",
    "            logits = model.hidden_state_to_token(hidden_states)\n",
    "            shifted_logits = logits[:, :-1, :] #take all but last token cause mismatch right now -> output pred 0, pred 1, pred 2\n",
    "            shifted_labels = labels[:,1:] #take all but first token                                labels word1 , word 2, word 3\n",
    "            loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)                                  #but we want pred 0 to predict word 1, pred 1 to predict word 2, pred 2 to predict word 3, so remove first label so labels look like [word2,word3..] and predictions remove last since there is no more word after last word.\n",
    "            loss = loss_fn(shifted_logits.reshape(-1,logits.size(-1)), shifted_labels.reshape(-1))  # flatten to [B*(seq_len-1), vocab_size] since N= B*(seq_len-1) for logits and [B*(seq_len-1)] for labels\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # raise NotImplementedError     \n",
    "            total_train_loss += loss.item()\n",
    "            total_train_steps += 1\n",
    "            global_train_avg_loss = total_train_loss / total_train_steps\n",
    "            global_train_losses.append(global_train_avg_loss)\n",
    "\n",
    "            loop.set_postfix({'avg_loss': f\"{global_train_avg_loss:.4f}\"})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished | Global Avg Loss: {global_train_avg_loss:.4f}\")\n",
    "\n",
    "        acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, dev_loader, max_gen_length=1, device=DEVICE)\n",
    "\n",
    "        \n",
    "        # Save model at every epoch\n",
    "        torch.save(model.state_dict(), f\"{SAVE_DIR}/model_{lang}_epoch_{epoch}.pt\")\n",
    "        # Save best model\n",
    "        if acc > best_dev_acc:\n",
    "            best_dev_acc = acc\n",
    "        #     torch.save(model.state_dict(), f\"{SAVE_DIR}/model_{lang}.pt\")\n",
    "        #     print(f\"New best model saved at {SAVE_DIR}/model_{lang}.pt with dev accuracy {best_dev_acc*100:.2f}%\")\n",
    "        # Or trigger early stopping\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "        # break\n",
    "        \n",
    "        \n",
    "def compute_fertility(dataset, tokenizer):\n",
    "    total_words = 0\n",
    "    total_tokens = 0\n",
    "    samples = len(dataset)\n",
    "    \n",
    "    for i in tqdm(range(samples), desc=\"Computing fertility\"):\n",
    "        row = dataset.data.iloc[i]\n",
    "        for sent in [row['premise'], row['hypo']]:\n",
    "            words = sent.strip().split()  # crude word estimate\n",
    "            tokens = tokenizer.tokenize(sent)\n",
    "            total_words += len(words)\n",
    "            total_tokens += len(tokens)\n",
    "    \n",
    "    fertility = total_tokens / total_words if total_words > 0 else 0.0\n",
    "    return fertility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd579afb-dda1-443d-a307-216d8cb40e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T20:05:16.673206Z",
     "iopub.status.busy": "2025-11-21T20:05:16.672985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='train', lang='vi', total=392702, subset=1, subset_count=392702\n",
      "Dataset initialized: split='dev', lang='vi', total=2490, subset=1, subset_count=2490\n",
      "Dataset initialized: split='test', lang='vi', total=5010, subset=1, subset_count=5010\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 108/49088 [00:17<3:24:20,  3.99it/s, avg_loss=9.7127]Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Training: 100%|██████████| 49088/49088 [2:23:05<00:00,  5.72it/s, avg_loss=0.9659]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished | Global Avg Loss: 0.9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 2490/2490 [00:23<00:00, 106.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 63.05%\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 49088/49088 [2:23:14<00:00,  5.71it/s, avg_loss=0.8816]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished | Global Avg Loss: 0.8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 2490/2490 [00:23<00:00, 107.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 64.70%\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 49088/49088 [2:23:14<00:00,  5.71it/s, avg_loss=0.8374]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished | Global Avg Loss: 0.8374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 2490/2490 [00:23<00:00, 107.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 66.35%\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 49088/49088 [2:23:03<00:00,  5.72it/s, avg_loss=0.8051]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished | Global Avg Loss: 0.8051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 2490/2490 [00:22<00:00, 109.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 66.39%\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 36087/49088 [1:45:00<37:08,  5.83it/s, avg_loss=0.7848]  "
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "import time\n",
    "\n",
    "# Empty cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Languages not supported: hindi, urdu, bulgurian, russian, greek, thai\n",
    "# Original list: ['ar','de','es','fr','sw','tr','vi','zh']\n",
    "chosen_lang = ['vi', 'zh', 'es'] # \"vi\", \"zh\", \"es\"\n",
    "all_results = {}\n",
    "\n",
    "for lang in chosen_lang:\n",
    "    # Load a pretrained GPT-2 model with official weights\n",
    "    model = GPT2Model.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "    \n",
    "    # Split train, val and test\n",
    "    train_dataset, dev_dataset, test_dataset = split_dataset(lang, tokenizer)\n",
    "    \n",
    "    # Full finetunign pipeline, refer to code above for params\n",
    "    start_time = time.time()\n",
    "    full_finetuning_pipeline(train_dataset, dev_dataset, lang)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Perform testing\n",
    "    path = f\"./{SAVE_DIR}/model_{lang}.pt\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval\n",
    "    test_loader = DataLoader(test_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "    acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
    "    fertility_score = compute_fertility(train_dataset, tokenizer)\n",
    "    time_taken = end_time - start_time\n",
    "    all_results[lang] = [acc, time_taken, fertility_score]\n",
    "    \n",
    "    print(f\"\\nZero-shot cross-lingual accuracy {lang}: {acc*100:.2f}%\")\n",
    "    print(f\"Training took {(time_taken)/60:.2f} minutes for {EPOCHS} epochs\")\n",
    "    print(f\"Fertility score: {fertility_score:.2f}\")\n",
    "    \n",
    "    # break\n",
    "    \n",
    "\n",
    "for lang, data in all_results.items():\n",
    "    # First data denotes accuracy\n",
    "    print(f\"\\nZero-shot cross-lingual accuracy {lang}: {data[0]*100:.2f}%\")\n",
    "    # Second denotes time\n",
    "    print(f\"Training took {(data[1])/60:.2f} minutes for {EPOCHS} epochs\")\n",
    "    # Third denotes fertility score\n",
    "    print(f\"Fertility score: {data[2]:.2f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703561b0-22b2-42a3-a186-9c321f039e9e",
   "metadata": {},
   "source": [
    "## Code-switched test with custom CSDataset & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e458a2f-8d43-44f2-8cd3-66dac8ddf8db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:22:07.270123Z",
     "iopub.status.busy": "2025-12-06T10:22:07.269403Z",
     "iopub.status.idle": "2025-12-06T10:22:07.282556Z",
     "shell.execute_reply": "2025-12-06T10:22:07.281896Z",
     "shell.execute_reply.started": "2025-12-06T10:22:07.270103Z"
    }
   },
   "outputs": [],
   "source": [
    "class CSDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        split=\"test\",\n",
    "        lang=\"zh\",\n",
    "        test_path=\"cs_dataset/cs_{lang}_test.tsv\",\n",
    "        tokenizer=None,\n",
    "        max_length=1024,\n",
    "        subset = 1.0  # 0~1\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.lang = lang\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.LABEL2ID = {\"entailment\": 0, \"contradictory\": 1, \"neutral\": 2}\n",
    "        self.ID2LABEL = {v: k for k, v in self.LABEL2ID.items()}\n",
    "\n",
    "        if split==\"test\":\n",
    "            path = test_path.format(lang=self.lang)\n",
    "            df = self.read_dataset_tsv(path, split)\n",
    "            keep_cols = ['sentence1', 'sentence2', 'gold_label']\n",
    "            df = df[keep_cols].dropna()\n",
    "            df.rename(columns={'sentence1':'premise','sentence2':'hypo','gold_label':'label'}, inplace=True)\n",
    "            df['label'] = df['label'].replace({'contradiction': 'contradictory'})\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of ['train','dev','test']\")\n",
    "        \n",
    "        original_num = len(df)\n",
    "        if subset < 1.0:\n",
    "            n = max(1, int(len(df) * subset))\n",
    "            df = df.iloc[:n].reset_index(drop=True)\n",
    "        subset_num = len(df)\n",
    "\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        print(f\"Dataset initialized: split='{split}', lang='{lang}', total={original_num}, subset={subset}, subset_count={subset_num}\")\n",
    "\n",
    "    def read_dataset_tsv(self, path, split):\n",
    "        if split == \"train\":\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines()\n",
    "            header = lines[0].split(\"\\t\")\n",
    "            data = []\n",
    "            for i, line in enumerate(lines[1:], start=2):\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == len(header):\n",
    "                    data.append(parts)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(parts)} cols → {parts[:2]}\")\n",
    "        else:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.reader(f, delimiter=\"\\t\")\n",
    "                rows = list(reader)\n",
    "            header = rows[0]\n",
    "            expected_cols = len(header)\n",
    "            data = []\n",
    "            for i, row in enumerate(rows[1:], start=2):\n",
    "                if len(row) == expected_cols:\n",
    "                    data.append(row)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(row)} cols → {row[:2]}\")\n",
    "        return pd.DataFrame(data, columns=header)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        premise = row['premise']\n",
    "        hypo = row['hypo']\n",
    "        label = row['label']\n",
    "        if self.lang == 'zh': # de-tokenize for Chinese\n",
    "            premise = premise.replace(\" \", \"\")\n",
    "            hypo = hypo.replace(\" \", \"\")\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            prefix = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            full_text = prefix + str(self.LABEL2ID[label])\n",
    "            tokenized = self.tokenizer(\n",
    "                full_text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "            prefix_ids = self.tokenizer(prefix).input_ids\n",
    "            labels_ids = tokenized['input_ids'].clone()\n",
    "            labels_ids[:len(prefix_ids)] = -100 # Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "            tokenized['labels'] = labels_ids\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "        else:\n",
    "            text = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d192b7-a6ec-4ed5-a8de-211a569642f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:22:46.506505Z",
     "iopub.status.busy": "2025-12-06T10:22:46.505947Z",
     "iopub.status.idle": "2025-12-06T10:26:12.815367Z",
     "shell.execute_reply": "2025-12-06T10:26:12.814457Z",
     "shell.execute_reply.started": "2025-12-06T10:22:46.506484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='test', lang='en', total=5010, subset=1, subset_count=5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:45<00:00, 110.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 83.23%\n",
      "Training took 0.75 minutes for all 5010 samples\n",
      "Dataset initialized: split='test', lang='vi', total=5010, subset=1, subset_count=5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:51<00:00, 97.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 65.67%\n",
      "Training took 0.86 minutes for all 5010 samples\n",
      "Dataset initialized: split='test', lang='zh', total=5010, subset=1, subset_count=5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:49<00:00, 100.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 65.47%\n",
      "Training took 0.83 minutes for all 5010 samples\n",
      "Dataset initialized: split='test', lang='es', total=5010, subset=1, subset_count=5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:47<00:00, 104.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 70.16%\n",
      "Training took 0.80 minutes for all 5010 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Load a pretrained GPT-2 model with official weights\n",
    "model = GPT2Model.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "\n",
    "# Then continue from checkpoint if training was disrupted\n",
    "model_filenames = [\"model_en_epoch_3\", \"model_vi_epoch_3\", \"model_zh_epoch_4\", \"model_es_epoch_4\"]\n",
    "\n",
    "\n",
    "# Testting per-language weights on XNLI dataset\n",
    "for index, lang in enumerate ([\"en\", \"vi\", \"zh\", \"es\"]):\n",
    "    model.load_state_dict(torch.load(f\"./best_model/{model_filenames[index]}.pt\"))\n",
    "    model.eval()\n",
    "    \n",
    "    test_dataset = XNLIDataset(\n",
    "        split=\"test\",\n",
    "        lang=lang,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=TEST_SUBSET\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    print(f\"Training took {time_taken/60:.2f} minutes for all {len(test_dataset)} samples\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676b9ce3-8d3a-4c89-bb34-4f54d2a9de98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:26:39.748406Z",
     "iopub.status.busy": "2025-12-06T10:26:39.748168Z",
     "iopub.status.idle": "2025-12-06T10:28:31.344769Z",
     "shell.execute_reply": "2025-12-06T10:28:31.344090Z",
     "shell.execute_reply.started": "2025-12-06T10:26:39.748390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='test', lang='vi', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:09<00:00, 110.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 68.30%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='vi', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:09<00:00, 107.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 48.00%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='vi', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:09<00:00, 108.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 38.90%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='vi', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:09<00:00, 109.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 49.00%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='zh', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:09<00:00, 104.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 44.00%\n",
      "Training took 0.16 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='zh', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:09<00:00, 110.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 37.10%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='zh', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:08<00:00, 117.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 45.50%\n",
      "Training took 0.14 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='zh', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:08<00:00, 114.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 42.60%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='es', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:09<00:00, 110.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 60.90%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='es', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:09<00:00, 110.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 39.00%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='es', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:08<00:00, 111.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 37.10%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='es', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:08<00:00, 113.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 57.70%\n",
      "Training took 0.15 minutes for all 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test per-language weights on code-switched dataset\n",
    "for lang in [\"vi\", \"zh\", \"es\"]:\n",
    "    for filename in model_filenames:\n",
    "        model.load_state_dict(torch.load(f\"./best_model/{filename}.pt\"))\n",
    "        model.eval()\n",
    "    \n",
    "        test_dataset = CSDataset(\n",
    "            split=\"test\",\n",
    "            lang=lang,\n",
    "            tokenizer=tokenizer,\n",
    "            subset=TEST_SUBSET\n",
    "        )\n",
    "                    \n",
    "        test_loader = DataLoader(test_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "\n",
    "        print(f\"Training took {time_taken/60:.2f} minutes for all {len(test_dataset)} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
