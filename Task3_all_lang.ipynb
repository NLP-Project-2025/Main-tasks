{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c74c28b",
   "metadata": {},
   "source": [
    "### This notebook contains contains code for Task 3 (All-language fine tuning & testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f2a28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:28.597186Z",
     "iopub.status.busy": "2025-12-06T10:36:28.596422Z",
     "iopub.status.idle": "2025-12-06T10:36:31.968860Z",
     "shell.execute_reply": "2025-12-06T10:36:31.968332Z",
     "shell.execute_reply.started": "2025-12-06T10:36:28.597162Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778f5e71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:31.970014Z",
     "iopub.status.busy": "2025-12-06T10:36:31.969840Z",
     "iopub.status.idle": "2025-12-06T10:36:36.915149Z",
     "shell.execute_reply": "2025-12-06T10:36:36.914439Z",
     "shell.execute_reply.started": "2025-12-06T10:36:31.969999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn, dtype\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from config import PretrainedConfig, GPT2Config\n",
    "from transformers import GPT2Model as OpenAIGPT2Model\n",
    "from transformers import GPT2Tokenizer\n",
    "from utils import *\n",
    "from einops import rearrange\n",
    "from typing import Callable, Iterable, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208bb62",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "Different from the official GPT-2 architecture; this one is without the language modelling head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6187ca22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:36.916667Z",
     "iopub.status.busy": "2025-12-06T10:36:36.916202Z",
     "iopub.status.idle": "2025-12-06T10:36:36.925423Z",
     "shell.execute_reply": "2025-12-06T10:36:36.924916Z",
     "shell.execute_reply.started": "2025-12-06T10:36:36.916642Z"
    }
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_attention_heads = config.num_attention_heads\n",
    "    self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "    self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "    # Initialize the linear transformation layers for key, value, query.\n",
    "    self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "    # This dropout is applied to normalized attention scores following the original\n",
    "    # implementation of transformer. Although it is a bit unusual, we empirically\n",
    "    # observe that it yields better performance.\n",
    "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "  def transform(self, x, linear_layer):\n",
    "    # The corresponding linear_layer of k, v, q are used to project the hidden_state (x).\n",
    "    proj = linear_layer(x)\n",
    "    # Next, we need to produce multiple heads for the proj. This is done by spliting the\n",
    "    # hidden state to self.num_attention_heads, each of size self.attention_head_size.\n",
    "    proj = rearrange(proj, 'b t (h d) -> b t h d', h=self.num_attention_heads)\n",
    "    # By proper transpose, we have proj of size [bs, num_attention_heads, seq_len, attention_head_size].\n",
    "    proj = rearrange(proj, 'b t h d -> b h t d')\n",
    "    return proj\n",
    "\n",
    "  def attention(self, key, query, value, attention_mask):\n",
    "    \"\"\"\n",
    "    TODO-1: Compute scaled dot-product attention for GPT-2.\n",
    "\n",
    "    Implementation hints:\n",
    "    1. Compute raw attention scores using QK^T, and scale them by sqrt(d_k).\n",
    "    2. Apply a causal mask (lower-triangular) to prevent attending to future tokens.\n",
    "    3. Optionally add the external attention_mask (e.g., padding positions).\n",
    "    4. Normalize the scores with softmax to obtain attention probabilities.\n",
    "    5. Apply dropout on the probabilities.\n",
    "    6. Use them to weight the values (V) and obtain the context vectors.\n",
    "    7. Finally, merge all attention heads back into a single hidden representation.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #query=(num_queries,d_k)\n",
    "    #compute raw attention scores\n",
    "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.attention_head_size)\n",
    "    #apply causal mask\n",
    "    #create the lower triangular matrix\n",
    "    seq_len = attention_scores.size(-1)\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), device=attention_scores.device))\n",
    "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0) #add batch and head dimensions\n",
    "    #apply mask, set positions with 0 to -inf\n",
    "    attention_scores = attention_scores.masked_fill(causal_mask == 0, -1e9)\n",
    "    #add external attention mask\n",
    "    if attention_mask is not None:\n",
    "      attention_scores = attention_scores + attention_mask\n",
    "    #normalize scores with softmax\n",
    "    attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "    #apply dropout\n",
    "    attention_probs = self.dropout(attention_probs)\n",
    "    #weigh the values(attention_prob@value)\n",
    "    context_vector=torch.matmul(attention_probs,value)\n",
    "    #merge all attention heads into 1\n",
    "    batch_size, num_heads, seq_len, head_size = context_vector.size()\n",
    "    context = context_vector.transpose(1, 2).contiguous().reshape(batch_size, seq_len, num_heads * head_size)\n",
    "\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "  def forward(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    hidden_states: [bs, seq_len, hidden_state]\n",
    "    attention_mask: [bs, 1, 1, seq_len]\n",
    "    output: [bs, seq_len, hidden_state]\n",
    "    \"\"\"\n",
    "    # First, we have to generate the key, value, query for each token for multi-head attention\n",
    "    # using self.transform (more details inside the function).\n",
    "    # Size of *_layer is [bs, num_attention_heads, seq_len, attention_head_size].\n",
    "    key_layer = self.transform(hidden_states, self.key)\n",
    "    value_layer = self.transform(hidden_states, self.value)\n",
    "    query_layer = self.transform(hidden_states, self.query)\n",
    "    \n",
    "    # Calculate the multi-head attention using the self.attention function.\n",
    "    attn_value = self.attention(key_layer, query_layer, value_layer, attention_mask)\n",
    "    return attn_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4234a0b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:36.926638Z",
     "iopub.status.busy": "2025-12-06T10:36:36.926206Z",
     "iopub.status.idle": "2025-12-06T10:36:36.933026Z",
     "shell.execute_reply": "2025-12-06T10:36:36.932448Z",
     "shell.execute_reply.started": "2025-12-06T10:36:36.926617Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPT2Layer(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    # Multi-head attention.\n",
    "    self.self_attention = CausalSelfAttention(config)\n",
    "    # Add-norm for multi-head attention.\n",
    "    self.attention_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.attention_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "    self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    # Feed forward.\n",
    "    self.interm_dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "    self.interm_af = F.gelu\n",
    "    # Add-norm for feed forward.\n",
    "    self.out_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    self.out_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "    self.out_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "  def add(self, input, output, dense_layer, dropout):\n",
    "    \"\"\"\n",
    "    TODO-2: Residual connection with dense projection and dropout.\n",
    "    \n",
    "    Implementation hints:\n",
    "    1. Project the 'output' through dense_layer.\n",
    "    2. Apply dropout to prevent overfitting.\n",
    "    3. Add the original 'input' (residual connection) to the processed output.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #project output through dense_layer\n",
    "    projected=dense_layer(output)\n",
    "    #apply dropout\n",
    "    projected=dropout(projected)\n",
    "    #add OG input to output\n",
    "    return input+projected\n",
    "\n",
    "\n",
    "  def forward(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    TODO-3: Forward pass of a GPT-2 layer.\n",
    "\n",
    "    Implementation hints:\n",
    "    ---- Self-Attention Block ----\n",
    "    1. LayerNorm the input for stability using self.attention_layer_norm.\n",
    "    2. Compute multi-head causal self-attention using self.self_attention.\n",
    "    3. Apply residual connection using self.add (dense_layer=self.attention_dense, dropout=self.attention_dropout).\n",
    "\n",
    "    ---- Feed Forward Block ----\n",
    "    4. LayerNorm the hidden_states from attention block using self.out_layer_norm.\n",
    "    5. Pass through a two-layer feed-forward network with activation:\n",
    "       self.interm_dense -> self.interm_af -> self.out_dense\n",
    "    6. Apply residual connection again using self.add (dense_layer=self.out_dense, dropout=self.out_dropout).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #1. layernorm the input\n",
    "    normed_input=self.attention_layer_norm(hidden_states)\n",
    "    #2. compute the self attention\n",
    "    attn_output=self.self_attention(normed_input,attention_mask)\n",
    "    #3. apply residual connection\n",
    "    hidden_states=self.add(hidden_states,attn_output,self.attention_dense,self.attention_dropout)\n",
    "    #4. layernorm the hidden states from attention blocks\n",
    "    normed_ff=self.out_layer_norm(hidden_states)\n",
    "    #5. pass through 2 layer feed forward\n",
    "    ff_output=self.interm_dense(normed_ff)\n",
    "    ff_output=self.interm_af(ff_output)\n",
    "    ff_output = self.out_dense(ff_output)\n",
    "    ff_output = self.out_dropout(ff_output)\n",
    "    #6. apply residual connection\n",
    "    hidden_states = hidden_states + ff_output\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ec16ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:39.048455Z",
     "iopub.status.busy": "2025-12-06T10:36:39.047609Z",
     "iopub.status.idle": "2025-12-06T10:36:39.052982Z",
     "shell.execute_reply": "2025-12-06T10:36:39.052547Z",
     "shell.execute_reply.started": "2025-12-06T10:36:39.048437Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPTPreTrainedModel(nn.Module):\n",
    "\n",
    "  def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    self.name_or_path = config.name_or_path\n",
    "\n",
    "  def init_weights(self):\n",
    "    # Initialize weights\n",
    "    self.apply(self._init_weights)\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "    \"\"\" Initialize the weights \"\"\"\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "      # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "      # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "      module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "      module.bias.data.zero_()\n",
    "      module.weight.data.fill_(1.0)\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "      module.bias.data.zero_()\n",
    "\n",
    "  @property\n",
    "  def dtype(self) -> dtype:\n",
    "    return get_parameter_dtype(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d1b9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:40.775042Z",
     "iopub.status.busy": "2025-12-06T10:36:40.774341Z",
     "iopub.status.idle": "2025-12-06T10:36:40.786184Z",
     "shell.execute_reply": "2025-12-06T10:36:40.785490Z",
     "shell.execute_reply.started": "2025-12-06T10:36:40.775017Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPT2Model(GPTPreTrainedModel):\n",
    "  \"\"\"\n",
    "  The GPT model returns the final embeddings for each token in a sentence.\n",
    "\n",
    "  The model consists of:\n",
    "  1. Embedding layers (used in self.embed).\n",
    "  2. A stack of n GPT layers (used in self.encode).\n",
    "  3. A linear transformation layer for the [CLS] token (used in self.forward, as given).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "    self.config = config\n",
    "\n",
    "    # Embedding layers.\n",
    "    self.word_embedding = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "    self.pos_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "    self.embed_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # Register position_ids (1, len position emb) to buffer because it is a constant.\n",
    "    position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)\n",
    "    self.register_buffer('position_ids', position_ids)\n",
    "\n",
    "    # GPT-2 layers.\n",
    "    self.gpt_layers = nn.ModuleList([GPT2Layer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    # [CLS] token transformations.\n",
    "    self.pooler_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.pooler_af = nn.Tanh()\n",
    "\n",
    "    # Final layer norm.\n",
    "    self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def embed(self, input_ids):\n",
    "    \"\"\"\n",
    "    TODO-4: Embedding layer of the GPT-2 model.\n",
    "\n",
    "    Implementation hints:\n",
    "    1. Use self.word_embedding to convert input_ids to embeddings.\n",
    "    2. Generate position ids and convert to embeddings using self.pos_embedding.\n",
    "    3. Sum token and position embeddings.\n",
    "    4. Apply self.embed_dropout to the sum.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    #get token embeddings\n",
    "    token_embeddings=self.word_embedding(input_ids)\n",
    "    batch_size, seq_len = input_ids.size()\n",
    "    #generate position ids and convert to embeddings\n",
    "    position_ids = self.position_ids[:, :seq_len]\n",
    "    position_ids = position_ids.expand(batch_size, seq_len)\n",
    "    position_embeddings = self.pos_embedding(position_ids)\n",
    "    #sum token and position embeddings\n",
    "    sum_of_embeddings=token_embeddings+position_embeddings\n",
    "    sum_of_embeddings=self.embed_dropout(sum_of_embeddings)\n",
    "    return sum_of_embeddings\n",
    "\n",
    "  def encode(self, hidden_states, attention_mask):\n",
    "    \"\"\"\n",
    "    hidden_states: the output from the embedding layer [batch_size, seq_len, hidden_size]\n",
    "    attention_mask: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    # Get the extended attention mask for self-attention.\n",
    "    # Returns extended_attention_mask of size [batch_size, 1, 1, seq_len].\n",
    "    # Distinguishes between non-padding tokens (with a value of 0) and padding tokens\n",
    "    # (with a value of a large negative number).\n",
    "    extended_attention_mask: torch.Tensor = get_extended_attention_mask(attention_mask, self.dtype)\n",
    "\n",
    "    # Pass the hidden states through the encoder layers.\n",
    "    for i, layer_module in enumerate(self.gpt_layers):\n",
    "      # Feed the encoding from the last bert_layer to the next.\n",
    "      hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    input_ids: [batch_size, seq_len], seq_len is the max length of the batch\n",
    "    attention_mask: same size as input_ids, 1 represents non-padding tokens, 0 represents padding tokens\n",
    "    \"\"\"\n",
    "    # Get the embedding for each input token.\n",
    "    embedding_output = self.embed(input_ids=input_ids)\n",
    "\n",
    "    # Feed to a transformer (a stack of GPTLayers).\n",
    "    sequence_output = self.encode(embedding_output, attention_mask=attention_mask)\n",
    "    sequence_output = self.final_layer_norm(sequence_output)\n",
    "\n",
    "    # Get the hidden state of the final token.\n",
    "    last_non_pad_idx = attention_mask.sum(dim=1) - 1  # Subtract 1 to get last index\n",
    "    last_token = sequence_output[torch.arange(sequence_output.shape[0]), last_non_pad_idx]\n",
    "\n",
    "    return {'last_hidden_state': sequence_output, 'last_token': last_token}\n",
    "\n",
    "  def hidden_state_to_token(self, hidden_state):\n",
    "    \"\"\"\n",
    "    TODO-5: Convert hidden states back to token logits.\n",
    "\n",
    "    Implementation hints: \n",
    "    - GPT-2 uses weight tying with the input word embeddings. \n",
    "    - The logits are the dot product between output hidden states and the word embedding weights: hidden_state(s) * E^T\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    logits = torch.matmul(hidden_state, self.word_embedding.weight.T)\n",
    "    return logits\n",
    "\n",
    "  @classmethod\n",
    "  def from_pretrained(cls, model='gpt2', d=768, l=12, num_heads=12):\n",
    "    gpt_model = OpenAIGPT2Model.from_pretrained(model).eval()\n",
    "    our_model = GPT2Model(GPT2Config(hidden_size=d, num_hidden_layers=l,num_attention_heads=num_heads,\n",
    "                                     intermediate_size=d*3)).eval()\n",
    "\n",
    "    # Load word and positional embeddings.\n",
    "    our_model.word_embedding.load_state_dict(gpt_model.wte.state_dict())\n",
    "    our_model.pos_embedding.load_state_dict(gpt_model.wpe.state_dict())\n",
    "\n",
    "    for i in range(l):\n",
    "      l = our_model.gpt_layers[i]\n",
    "      # Remap the Q,K,V weights from a conv1d to 3 linear projections\n",
    "      l.self_attention.query.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, :d].T\n",
    "      l.self_attention.query.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][:d]\n",
    "      l.self_attention.key.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d:d*2].T\n",
    "      l.self_attention.key.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d:d*2]\n",
    "      l.self_attention.value.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.weight'][:, d*2:].T\n",
    "      l.self_attention.value.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_attn.bias'][d*2:]\n",
    "\n",
    "      # Remap final dense layer in MHA.\n",
    "      l.attention_dense.weight.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.weight'].T\n",
    "      l.attention_dense.bias.data = gpt_model.state_dict()[f'h.{i}.attn.c_proj.bias']\n",
    "\n",
    "      # Remap attention layer norm.\n",
    "      l.attention_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_1.weight']\n",
    "      l.attention_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_1.bias']\n",
    "\n",
    "      # Remap post-attention MLP layers.\n",
    "      l.interm_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.weight'].T\n",
    "      l.interm_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_fc.bias']\n",
    "      l.out_dense.weight.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.weight'].T\n",
    "      l.out_dense.bias.data = gpt_model.state_dict()[f'h.{i}.mlp.c_proj.bias']\n",
    "\n",
    "      # Remap second layer norm weights.\n",
    "      l.out_layer_norm.weight.data = gpt_model.state_dict()[f'h.{i}.ln_2.weight']\n",
    "      l.out_layer_norm.bias.data = gpt_model.state_dict()[f'h.{i}.ln_2.bias']\n",
    "\n",
    "    # Remap the final layer norm values.\n",
    "    our_model.final_layer_norm.weight.data = gpt_model.state_dict()['ln_f.weight']\n",
    "    our_model.final_layer_norm.bias.data = gpt_model.state_dict()['ln_f.bias']\n",
    "\n",
    "    return our_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62773f",
   "metadata": {},
   "source": [
    "## Adam Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8ea6857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:45.329598Z",
     "iopub.status.busy": "2025-12-06T10:36:45.329081Z",
     "iopub.status.idle": "2025-12-06T10:36:45.338800Z",
     "shell.execute_reply": "2025-12-06T10:36:45.338142Z",
     "shell.execute_reply.started": "2025-12-06T10:36:45.329569Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            params: Iterable[torch.nn.parameter.Parameter],\n",
    "            lr: float = 1e-3,\n",
    "            betas: Tuple[float, float] = (0.9, 0.999),\n",
    "            eps: float = 1e-6,\n",
    "            weight_decay: float = 0.0,\n",
    "            correct_bias: bool = True,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Callable = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                # State should be stored in this dictionary.\n",
    "                state = self.state[p]\n",
    "\n",
    "                # Access hyperparameters from the `group` dictionary.\n",
    "                lr = group[\"lr\"]\n",
    "                eps = group[\"eps\"]\n",
    "                weight_decay = group[\"weight_decay\"]\n",
    "                correct_bias = group[\"correct_bias\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                \n",
    "                state[\"step\"] += 1\n",
    "                t = state[\"step\"]\n",
    "                \n",
    "                \"\"\"\n",
    "                TODO-6: Implement the AdamW parameter update for this step.\n",
    "\n",
    "                Implementation hints:\n",
    "                1. Update biased first moment estimate:\n",
    "                    m_t = beta1 * m_{t-1} + (1 - beta1) * grad\n",
    "                2. Update biased second raw moment estimate:\n",
    "                    v_t = beta2 * v_{t-1} + (1 - beta2) * grad^2\n",
    "                3. Apply bias correction if correct_bias=True:\n",
    "                    m_hat = m_t / (1 - beta1^t)\n",
    "                    v_hat = v_t / (1 - beta2^t)\n",
    "                4. Compute step size:\n",
    "                    step_size = lr (or lr / (1 - beta1^t) if bias correction)\n",
    "                5. Update parameters:\n",
    "                    p = p - step_size * m_hat / (sqrt(v_hat) + eps)\n",
    "                6. Apply decoupled weight decay after the parameter update (if weight_decay > 0):\n",
    "                    p = p - lr * weight_decay * p\n",
    "                Reference:\n",
    "                Algorithm 1 in \"Adam: A Method for Stochastic Optimization\"\n",
    "                https://arxiv.org/abs/1412.6980\n",
    "                \"\"\"\n",
    "                ### YOUR CODE HERE\n",
    "                exp_avg = state[\"exp_avg\"]\n",
    "                exp_avg_sq = state[\"exp_avg_sq\"]\n",
    "\n",
    "                # update biased first and second moments\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # bias correction\n",
    "                if correct_bias:\n",
    "                    m_hat = exp_avg / (1 - beta1 ** t)\n",
    "                    v_hat = exp_avg_sq / (1 - beta2 ** t)\n",
    "                else:\n",
    "                    m_hat = exp_avg\n",
    "                    v_hat = exp_avg_sq\n",
    "\n",
    "                # update p\n",
    "                p.data.addcdiv_(-lr, m_hat, torch.sqrt(v_hat) + eps)\n",
    "\n",
    "                # apply weight decay\n",
    "                if weight_decay > 0:\n",
    "                    p.data.add_(-lr * weight_decay, p.data)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397b223",
   "metadata": {},
   "source": [
    "## Text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8e64e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:49.425226Z",
     "iopub.status.busy": "2025-12-06T10:36:49.424455Z",
     "iopub.status.idle": "2025-12-06T10:36:49.430046Z",
     "shell.execute_reply": "2025-12-06T10:36:49.429516Z",
     "shell.execute_reply.started": "2025-12-06T10:36:49.425202Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_gpt2(model, tokenizer, input_ids, max_gen_length=50, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate text from a GPT-2 model given a single input sequence (greedy decoding).\n",
    "\n",
    "    Note:\n",
    "        - Currently only supports batch_size=1 (single input sequence).\n",
    "        - Using greedy decoding, so each run with the same input produces the same output.\n",
    "        - Other sampling-based decoding methods (e.g., top-k, top-p, temperature) can introduce randomness and yield different outputs each run.\n",
    "\n",
    "    Args:\n",
    "        model: GPT-2 model (pretrained or fine-tuned)\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        input_ids: torch.LongTensor of shape [1, seq_len], input token IDs\n",
    "        max_gen_length: int, maximum number of tokens to generate\n",
    "        device: str, \"cuda\" or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)  # move input to device\n",
    "    output_ids = input_ids.clone()\n",
    "\n",
    "    \"\"\"\n",
    "    TODO-8: Greedy next-token generation loop\n",
    "\n",
    "    Implementation hints:\n",
    "    Repeat the below steps up to max_gen_length:\n",
    "    1. Construct an attention mask based on current output_ids (non-pad tokens).\n",
    "    2. Pass output_ids and attention_mask through the model to get hidden states.\n",
    "    3. Convert the last hidden state to logits over the vocabulary using model.hidden_state_to_token.\n",
    "    4. Select the next token using greedy decoding (argmax over logits).\n",
    "    5. Append the next token to output_ids.\n",
    "    6. Stop the loop early if the EOS token is generated.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    for _ in range(max_gen_length):\n",
    "        attention_mask=torch.ones_like(output_ids)\n",
    "        hidden_states=model(output_ids,attention_mask) #hidden state is a dictionary w keys last_hidden_state and last_token\n",
    "        # print(type(hidden_states))\n",
    "        # print(hidden_states.keys() if hasattr(hidden_states, 'keys') else hidden_states)\n",
    "        last_hidden = hidden_states['last_hidden_state'][:, -1, :] #shape : [batch_size, sequence_length, hidden_dim]\n",
    "        logits = model.hidden_state_to_token(last_hidden)\n",
    "        next_token = torch.argmax(logits,dim=-1).unsqueeze(-1)\n",
    "        output_ids = torch.cat([output_ids, next_token],dim=-1)\n",
    "        if (next_token == tokenizer.eos_token_id).any():\n",
    "            break\n",
    "\n",
    "    # raise NotImplementedError\n",
    "\n",
    "    # Decode generated tokens to string\n",
    "    ids = output_ids[0]\n",
    "    text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a354c",
   "metadata": {},
   "source": [
    "## Loading NLI Dataset\n",
    "Where langauges are appended together in a single big dataset for train, dev and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b2019e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:53.519993Z",
     "iopub.status.busy": "2025-12-06T10:36:53.519535Z",
     "iopub.status.idle": "2025-12-06T10:36:53.535980Z",
     "shell.execute_reply": "2025-12-06T10:36:53.535404Z",
     "shell.execute_reply.started": "2025-12-06T10:36:53.519973Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "    correct = sum(p.lower().strip() == l.lower().strip() for p, l in zip(preds, labels))\n",
    "    return correct / len(labels)\n",
    "\n",
    "def evaluate_gpt2_xnli(model, tokenizer, dataloader, max_gen_length=10, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader, desc=\"Generating\"):\n",
    "            input_ids = item['input_ids']\n",
    "            gen_text = generate_gpt2(model, tokenizer, input_ids, max_gen_length=max_gen_length, device=device)\n",
    "            pred_label = gen_text.split(\"Label:\")[-1].strip()\n",
    "            all_preds.append(pred_label)\n",
    "            all_labels.extend(item['label_strs'])\n",
    "    acc = compute_accuracy(all_preds, all_labels)\n",
    "    print(f\"Evaluation accuracy: {acc*100:.2f}%\")\n",
    "    return acc, all_preds, all_labels\n",
    "\n",
    "class XNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for XNLI (Cross-lingual Natural Language Inference) task.\n",
    "\n",
    "    Supports train, dev, and test splits in a specific language, \n",
    "    tokenizes text inputs for GPT-style models, and optionally subsamples the dataset.\n",
    "\n",
    "    Attributes:\n",
    "        split (str): Dataset split, one of 'train', 'dev', 'test'.\n",
    "        lang (str): Language code (e.g., 'en', 'zh').\n",
    "        tokenizer: A HuggingFace tokenizer to convert text to input IDs.\n",
    "        max_length (int): Maximum sequence length for tokenization.\n",
    "        LABEL2ID (dict): Mapping from textual labels to integer IDs.\n",
    "        ID2LABEL (dict): Reverse mapping from integer IDs to textual labels.\n",
    "        data (pd.DataFrame): The loaded and preprocessed dataset.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        split=\"train\",\n",
    "        langs=[\"en\"], # Consider list\n",
    "        train_path_template=\"XNLI-MT-1.0/multinli/multinli.train.{lang}.tsv\",\n",
    "        test_path=\"XNLI-1.0/xnli.test.tsv\",\n",
    "        dev_path=\"XNLI-1.0/xnli.dev.tsv\",\n",
    "        tokenizer=None,\n",
    "        max_length=1024,\n",
    "        subset = 1.0  # 0~1\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.langs = langs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.LABEL2ID = {\"entailment\": 0, \"contradictory\": 1, \"neutral\": 2}\n",
    "        self.ID2LABEL = {v: k for k, v in self.LABEL2ID.items()}\n",
    "\n",
    "        if split == \"train\":\n",
    "            dfs = []\n",
    "            for lang in langs:\n",
    "                path = train_path_template.format(lang=lang)\n",
    "                df_lang = self.read_xnli_tsv(path, split)\n",
    "                df_lang = df_lang.dropna(subset=['premise','hypo','label'])\n",
    "                dfs.append(df_lang)\n",
    "                \n",
    "                # de-tokenize for Chinese\n",
    "                if lang == 'zh': \n",
    "                    df_lang['premise'] = df_lang['premise'].str.replace(\" \", \"\")\n",
    "                    df_lang['hypo'] = df_lang['hypo'].str.replace(\" \", \"\")\n",
    "                    \n",
    "                print(f\"Loaded {lang}: {len(df_lang)} rows\")\n",
    "                \n",
    "            # Combine all languages\n",
    "            df = pd.concat(dfs).reset_index(drop=True)\n",
    "            print(f\"Total number rows: {len(df)}\")\n",
    "            \n",
    "        elif split in [\"dev\", \"test\"]:\n",
    "            path = test_path if split==\"test\" else dev_path\n",
    "            df = self.read_xnli_tsv(path, split)\n",
    "            # Obtain and append all languages in the given list\n",
    "            df = df[df['language'].isin(langs)].copy()\n",
    "            keep_cols = ['sentence1', 'sentence2', 'gold_label']\n",
    "            df = df[keep_cols].dropna()\n",
    "            df.rename(columns={'sentence1':'premise','sentence2':'hypo','gold_label':'label'}, inplace=True)\n",
    "            df['label'] = df['label'].replace({'contradiction': 'contradictory'})\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of ['train','dev','test']\")\n",
    "        \n",
    "        original_num = len(df)\n",
    "        if subset < 1.0:\n",
    "            # Randomly sample dataset, especially for a mixture of languages\n",
    "            n = max(1, int(len(df) * subset))\n",
    "            df = df.sample(n=n, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        subset_num = len(df)\n",
    "\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        print(f\"Dataset initialized: split='{split}', lang='{langs}', total={original_num}, subset={subset}, subset_count={subset_num}\")\n",
    "\n",
    "    def read_xnli_tsv(self, path, split):\n",
    "        \"\"\"\n",
    "        Read an XNLI TSV file and return it as a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to the TSV file.\n",
    "            split (str): One of \"train\", \"dev\", \"test\" indicating the dataset split.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The dataset as a DataFrame with appropriate columns.\n",
    "        \"\"\"\n",
    "        if split == \"train\":\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines()\n",
    "            header = lines[0].split(\"\\t\")\n",
    "            data = []\n",
    "            for i, line in enumerate(lines[1:], start=2):\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == len(header):\n",
    "                    data.append(parts)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(parts)} cols → {parts[:2]}\")\n",
    "        else:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.reader(f, delimiter=\"\\t\")\n",
    "                rows = list(reader)\n",
    "            header = rows[0]\n",
    "            expected_cols = len(header)\n",
    "            data = []\n",
    "            for i, row in enumerate(rows[1:], start=2):\n",
    "                if len(row) == expected_cols:\n",
    "                    data.append(row)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(row)} cols → {row[:2]}\")\n",
    "        return pd.DataFrame(data, columns=header)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of examples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single example by index and tokenize it.\n",
    "\n",
    "        For training split:\n",
    "            - Constructs the input as \"Premise: ... Hypothesis: ... Label: ...\"\n",
    "            - Tokenizes the full input.\n",
    "            - Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "\n",
    "        For dev/test split:\n",
    "            - Constructs the input without label as \"Premise: ... Hypothesis: ... Label:\"\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains 'input_ids', 'attention_mask', 'labels' (train only), 'label_str'\n",
    "        \"\"\"\n",
    "        row = self.data.iloc[idx]\n",
    "        premise = row['premise']\n",
    "        hypo = row['hypo']\n",
    "        label = row['label']\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            prefix = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            full_text = prefix + str(self.LABEL2ID[label])\n",
    "            tokenized = self.tokenizer(\n",
    "                full_text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "            prefix_ids = self.tokenizer(prefix).input_ids\n",
    "            labels_ids = tokenized['input_ids'].clone()\n",
    "            labels_ids[:len(prefix_ids)] = -100 # Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "            tokenized['labels'] = labels_ids\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "        else:\n",
    "            text = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Collate a batch of examples into padded tensors.\n",
    "\n",
    "        Pads 'input_ids' and 'attention_mask' to the max length in the batch.\n",
    "        Pads 'labels' with -100 if present.\n",
    "        Collects 'label_str' for reference.\n",
    "\n",
    "        Returns:\n",
    "            dict: Padded tensors and label strings for the batch.\n",
    "        \"\"\"\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            [b['input_ids'] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            [b['attention_mask'] for b in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "\n",
    "        if 'labels' in batch[0]:\n",
    "            labels = torch.nn.utils.rnn.pad_sequence(\n",
    "                [b['labels'] for b in batch],\n",
    "                batch_first=True,\n",
    "                padding_value=-100\n",
    "            )\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        label_strs = [b['label_str'] for b in batch]\n",
    "\n",
    "        out = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label_strs\": label_strs}\n",
    "        if labels is not None:\n",
    "            out[\"labels\"] = labels\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f891ef",
   "metadata": {},
   "source": [
    "## Fine-tune GPT-2 (all language)\n",
    "\n",
    "Guidance: Load the pretrained GPT-2 (again, not the ones fine-tuned on English NLI) along with the training data for all target languages, including English. \n",
    "\n",
    "For non-English languages, select those that performed well in the zero-shot cross-lingual transfer and fertility evaluation. It depends on you how many languages to include. \n",
    "\n",
    "Fine-tune a single model on this combined multilingual dataset. Afterwards, compare this model with the per-language fine-tuned models and the zero-shot cross-lingual transfer results.\n",
    "\n",
    "### Generic retraining pipeline\n",
    "1. Load dataset from XNLIDataset and split it into test and train.\n",
    "2. Use DataLoader to load it up.\n",
    "3. Use Adam optimiser from the function AdamW(Optimizer).\n",
    "4. Build the training loop (intitialise weights, train, get loss, gradietn descent, early stopping based on dev_dataset accuracy/results).\n",
    "5. Save the best checkpoint per langauge\n",
    "6. Evaluate via test_set\n",
    "7. Then comapre with its zero-shot counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ed5ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:36:59.553796Z",
     "iopub.status.busy": "2025-12-06T10:36:59.553151Z",
     "iopub.status.idle": "2025-12-06T10:37:00.397066Z",
     "shell.execute_reply": "2025-12-06T10:37:00.396642Z",
     "shell.execute_reply.started": "2025-12-06T10:36:59.553773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd975eb269e94fb8afbcd5378a672e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b91ba548ded4118958ae0dd81f06706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7d556f8c8340e784ef72c6ce4a4d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5aa2e90a9347d6b74fccbd9ccd4ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e17db1092b34a92ad6dd64c2a8e9891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Adjust from 0 to 1, but consider 0.1 for rough testing of pipeline\n",
    "TRAIN_SUBSET = 1 #0.001 for testing, 1 for actual fine-tuning\n",
    "DEV_SUBSET = 1\n",
    "TEST_SUBSET = 1\n",
    "\n",
    "# Hyperparamter of gpt2 fine-tuning\n",
    "EPOCHS = 5 # 2 for testing, 1 for actual fine-tuning\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO=0.1\n",
    "CORRECT_BIAS = True\n",
    "SAVE_DIR = \"best_model\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "def split_dataset(langs, tokenizer):\n",
    "    train_dataset = XNLIDataset(\n",
    "        split=\"train\",\n",
    "        langs=langs,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=TRAIN_SUBSET\n",
    "    )\n",
    "\n",
    "    dev_dataset = XNLIDataset(\n",
    "        split=\"dev\",\n",
    "        langs=langs,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=DEV_SUBSET\n",
    "    )\n",
    "\n",
    "    test_dataset = XNLIDataset(\n",
    "        split=\"test\",\n",
    "        langs=langs,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=TEST_SUBSET\n",
    "    )\n",
    "    \n",
    "    return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "\n",
    "def full_finetuning_pipeline(train_dataset, dev_dataset, lang, start_epoch=0):\n",
    "    # Track best dev accuracy for model saving\n",
    "    # This only works for epoch > 1 \n",
    "    best_dev_acc = 0.0\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Create DataLoaders for training and validation datasets\n",
    "    train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=XNLIDataset.collate_fn)\n",
    "    dev_loader = DataLoader(dev_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "\n",
    "    # Scheduler is on by default\n",
    "    scheduler = None\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    # Initialize optimizer & scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, correct_bias=CORRECT_BIAS)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    # Load from checkpoint if possible\n",
    "    if start_epoch > 0:\n",
    "        optimizer_ckpt = f\"{SAVE_DIR}/optimizer_{lang}_epoch_{start_epoch}.pt\"\n",
    "        scheduler_ckpt = f\"{SAVE_DIR}/scheduler_{lang}_epoch_{start_epoch}.pt\"\n",
    "        if os.path.exists(optimizer_ckpt) and os.path.exists(scheduler_ckpt):\n",
    "            print(f\"Loading optimizer and scheduler from epoch {start_epoch} checkpoint...\")\n",
    "            optimizer.load_state_dict(torch.load(optimizer_chkpt))\n",
    "            scheduler.load_state_dict(torch.load(scheduler_ckpt))\n",
    "    else:\n",
    "        print(\"Checkpoint not found. Starting optimizer and scheduler from scratch.\")\n",
    "        \n",
    "    # Include early stopping\n",
    "    patience = 2\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    # Track training progress\n",
    "    global_train_losses = []\n",
    "    total_train_loss = 0.0\n",
    "    total_train_steps = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        model.train()\n",
    "        # Iterate over batches\n",
    "        loop = tqdm(train_loader, desc=\"Training\")\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)        # [B, seq_len]\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch.get(\"labels\").to(DEVICE)                    # [B, seq_len]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            hidden_states = model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']  # [B, seq_len, hidden]\n",
    "\n",
    "            \"\"\"\n",
    "            Reused from to-do 9.\n",
    "\n",
    "            Implementation hints:\n",
    "            1. Convert hidden states to logits over the vocabulary using model.hidden_state_to_token.\n",
    "            2. Shift logits and labels for next-token prediction to align each prediction with the correct next token.\n",
    "            3. Compute the cross-entropy loss, making sure positions with label=-100 are ignored.\n",
    "            4. Backpropagate and update model parameters.\n",
    "            \"\"\"\n",
    "\n",
    "            ### YOUR CODE HERE\n",
    "            logits = model.hidden_state_to_token(hidden_states)\n",
    "            shifted_logits = logits[:, :-1, :] #take all but last token cause mismatch right now -> output pred 0, pred 1, pred 2\n",
    "            shifted_labels = labels[:,1:] #take all but first token                                labels word1 , word 2, word 3\n",
    "            loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)                                  #but we want pred 0 to predict word 1, pred 1 to predict word 2, pred 2 to predict word 3, so remove first label so labels look like [word2,word3..] and predictions remove last since there is no more word after last word.\n",
    "            loss = loss_fn(shifted_logits.reshape(-1,logits.size(-1)), shifted_labels.reshape(-1))  # flatten to [B*(seq_len-1), vocab_size] since N= B*(seq_len-1) for logits and [B*(seq_len-1)] for labels\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # raise NotImplementedError     \n",
    "            total_train_loss += loss.item()\n",
    "            total_train_steps += 1\n",
    "            global_train_avg_loss = total_train_loss / total_train_steps\n",
    "            global_train_losses.append(global_train_avg_loss)\n",
    "\n",
    "            loop.set_postfix({'avg_loss': f\"{global_train_avg_loss:.4f}\"})\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished | Global Avg Loss: {global_train_avg_loss:.4f}\")\n",
    "        \n",
    "        # Save model at every epoch\n",
    "        torch.save(model.state_dict(), f\"{SAVE_DIR}/model_multi_epoch_{epoch}.pt\")\n",
    "        # Save optimiser at every epoch\n",
    "        torch.save(optimizer.state_dict(), f\"{SAVE_DIR}/optimizer_{lang}_epoch_{epoch}.pt\")\n",
    "        # Save scheduler as well\n",
    "        torch.save(scheduler.state_dict(), f\"{SAVE_DIR}/scheduler_{lang}_epoch_{epoch}.pt\")\n",
    "\n",
    "        acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, dev_loader, max_gen_length=1, device=DEVICE)\n",
    "\n",
    "        \n",
    "        # Save best model\n",
    "        if acc > best_dev_acc:\n",
    "            best_dev_acc = acc\n",
    "        # Or trigger early stopping\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                return\n",
    "        \n",
    "        \n",
    "def compute_fertility(dataset, tokenizer):\n",
    "    total_words = 0\n",
    "    total_tokens = 0\n",
    "    samples = len(dataset)\n",
    "    \n",
    "    for i in tqdm(range(samples), desc=\"Computing fertility\"):\n",
    "        row = dataset.data.iloc[i]\n",
    "        for sent in [row['premise'], row['hypo']]:\n",
    "            words = sent.strip().split()  # crude word estimate\n",
    "            tokens = tokenizer.tokenize(sent)\n",
    "            total_words += len(words)\n",
    "            total_tokens += len(tokens)\n",
    "    \n",
    "    fertility = total_tokens / total_words if total_words > 0 else 0.0\n",
    "    return fertility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c4b63-2977-408e-8ed5-cbecd75f2805",
   "metadata": {},
   "source": [
    "Note: Uncomment certain lines of code for fine-tuning. It's the same code cell used for pre-training so the logs were lost but I do save the logs in a text file 😭.\n",
    "\n",
    "```shell\n",
    "Loaded en: 392702 rows\n",
    "Loaded vi: 392702 rows\n",
    "Loaded zh: 392702 rows\n",
    "Loaded es: 392702 rows\n",
    "Total number rows: 1570808\n",
    "Dataset initialized: split='train', lang='['en', 'vi', 'zh', 'es']', total=1570808, subset=1, subset_count=1570808\n",
    "Dataset initialized: split='dev', lang='['en', 'vi', 'zh', 'es']', total=9960, subset=1, subset_count=9960\n",
    "Dataset initialized: split='test', lang='['en', 'vi', 'zh', 'es']', total=20040, subset=1, subset_count=20040\n",
    "Epoch 1/5\n",
    "Training:   0%|          | 320/196351 [00:42<8:06:57,  6.71it/s, avg_loss=9.3432]Token indices sequence length is longer than the specified maximum sequence length for this model (1737 > 1024). Running this sequence through the model will result in indexing errors\n",
    "Training: 100%|██████████| 196351/196351 [7:12:53<00:00,  7.56it/s, avg_loss=0.8379]  \n",
    "Epoch 1 finished | Global Avg Loss: 0.8379\n",
    "Generating: 100%|██████████| 9960/9960 [01:32<00:00, 107.72it/s]\n",
    "Evaluation accuracy: 68.61%\n",
    "Epoch 2/5\n",
    "Training: 100%|██████████| 196351/196351 [7:09:40<00:00,  7.62it/s, avg_loss=0.7056]3]\n",
    "Epoch 3 finished | Global Avg Loss: 0.7056\n",
    "Generating: 100%|██████████| 9960/9960 [01:33<00:00, 106.30it/s]\n",
    "Evaluation accuracy: 72.81%\n",
    "Early stopping triggered.\n",
    "Epoch 4/5\n",
    "Training:   2%|▏         | 3554/196351 [07:48<6:56:01,  7.72it/s, avg_loss=0.7047] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb5af6-7573-4c24-aa74-772622a7d456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T07:37:42.710589Z",
     "iopub.status.busy": "2025-12-06T07:37:42.710356Z",
     "iopub.status.idle": "2025-12-06T07:41:00.708521Z",
     "shell.execute_reply": "2025-12-06T07:41:00.707904Z",
     "shell.execute_reply.started": "2025-12-06T07:37:42.710574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='test', lang='['en', 'vi', 'zh', 'es']', total=20040, subset=1, subset_count=20040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 20040/20040 [03:13<00:00, 103.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 72.56%\n",
      "Training took 3.22 minutes for all 20040 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "import time\n",
    "\n",
    "# Empty cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "chosen_langs = [\"en\", \"vi\", \"zh\", \"es\"]\n",
    "\n",
    "\n",
    "# Load a pretrained GPT-2 model with official weights\n",
    "model = GPT2Model.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "\n",
    "# Then continue from checkpoint if training was disrupted\n",
    "model.load_state_dict(torch.load(\"./best_model/model_multi_epoch_2.pt\"))\n",
    "\n",
    "# Uncomment this for training\n",
    "# Split train, val and test\n",
    "# train_dataset, dev_dataset, test_dataset = split_dataset(chosen_langs, tokenizer)\n",
    "# print(train_dataset.data.head()) # Double check if it's a mixed language\n",
    "\n",
    "# Full finetunign pipeline, refer to code above for params\\\n",
    "# full_finetuning_pipeline(train_dataset, dev_dataset, lang=chosen_langs, start_epoch=4)\n",
    "\n",
    "# Uncomment this for testing\n",
    "model.eval()\n",
    "test_dataset = XNLIDataset(\n",
    "        split=\"test\",\n",
    "        langs=chosen_langs,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=TEST_SUBSET\n",
    "    )\n",
    "test_loader = DataLoader(test_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Training took {time_taken/60:.2f} minutes for all {len(test_dataset)} samples\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a59e5-e28b-4316-a655-2f0706cd2e61",
   "metadata": {},
   "source": [
    "Comparing with another weight from earlier epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18997db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T07:30:25.736869Z",
     "iopub.status.busy": "2025-12-06T07:30:25.736621Z",
     "iopub.status.idle": "2025-12-06T07:33:41.743200Z",
     "shell.execute_reply": "2025-12-06T07:33:41.742656Z",
     "shell.execute_reply.started": "2025-12-06T07:30:25.736852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='test', lang='['en', 'vi', 'zh', 'es']', total=20040, subset=1, subset_count=20040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 20040/20040 [03:11<00:00, 104.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 70.97%\n",
      "Training took 3.20 minutes for all 20040 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "import time\n",
    "\n",
    "# Empty cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "chosen_langs = [\"en\", \"vi\", \"zh\", \"es\"]\n",
    "\n",
    "\n",
    "# Load a pretrained GPT-2 model with official weights\n",
    "model = GPT2Model.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "\n",
    "# Then continue from checkpoint if training was disrupted\n",
    "model.load_state_dict(torch.load(\"./best_model/model_multi_epoch_4.pt\"))\n",
    "\n",
    "\n",
    "# Uncomment this for testing\n",
    "model.eval()\n",
    "test_dataset = XNLIDataset(\n",
    "        split=\"test\",\n",
    "        langs=chosen_langs,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=TEST_SUBSET\n",
    "    )\n",
    "test_loader = DataLoader(test_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Training took {time_taken/60:.2f} minutes for all {len(test_dataset)} samples\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f99413-d6aa-4e9e-820b-542862f81fb5",
   "metadata": {},
   "source": [
    "## Per language testing of multilingual weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e09f2785-cac2-4f6f-9427-34a2adbc0bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:28:48.502774Z",
     "iopub.status.busy": "2025-12-06T10:28:48.502545Z",
     "iopub.status.idle": "2025-12-06T10:32:10.520209Z",
     "shell.execute_reply": "2025-12-06T10:32:10.519441Z",
     "shell.execute_reply.started": "2025-12-06T10:28:48.502758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='test', lang='['en']', total=5010, subset=1, subset_count=5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:45<00:00, 109.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 83.31%\n",
      "Training took 0.76 minutes for all 5010 samples\n",
      "Dataset initialized: split='test', lang='['vi']', total=5010, subset=1, subset_count=5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:52<00:00, 95.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 68.34%\n",
      "Training took 0.88 minutes for all 5010 samples\n",
      "Dataset initialized: split='test', lang='['zh']', total=5010, subset=1, subset_count=5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:49<00:00, 101.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 66.13%\n",
      "Training took 0.82 minutes for all 5010 samples\n",
      "Dataset initialized: split='test', lang='['es']', total=5010, subset=1, subset_count=5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 5010/5010 [00:47<00:00, 104.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 72.46%\n",
      "Training took 0.80 minutes for all 5010 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Load a pretrained GPT-2 model with official weights\n",
    "model = GPT2Model.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "\n",
    "# Then continue from checkpoint if training was disrupted\n",
    "model.load_state_dict(torch.load(\"./best_model/model_multi_epoch_2.pt\"))\n",
    "model.eval()\n",
    "\n",
    "for lang in [\"en\", \"vi\", \"zh\", \"es\"]:\n",
    "    chosen_langs = [lang]\n",
    "    \n",
    "    test_dataset = XNLIDataset(\n",
    "            split=\"test\",\n",
    "            langs=chosen_langs,\n",
    "            tokenizer=tokenizer,\n",
    "            subset=TEST_SUBSET\n",
    "        )\n",
    "                    \n",
    "    test_loader = DataLoader(test_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "\n",
    "    start_time = time.time()\n",
    "    acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    print(f\"Training took {time_taken/60:.2f} minutes for all {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851edaf-1ce1-4529-8e0c-07bd24e0d684",
   "metadata": {},
   "source": [
    "## Code-switched test with custom CSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd3f8f1-a5fe-496c-a31e-f95f2a2ec15e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:37:07.992835Z",
     "iopub.status.busy": "2025-12-06T10:37:07.992435Z",
     "iopub.status.idle": "2025-12-06T10:37:08.004474Z",
     "shell.execute_reply": "2025-12-06T10:37:08.003805Z",
     "shell.execute_reply.started": "2025-12-06T10:37:07.992835Z"
    }
   },
   "outputs": [],
   "source": [
    "class CSDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        split=\"test\",\n",
    "        lang=\"zh\",\n",
    "        test_path=\"cs_dataset/cs_{lang}_test.tsv\",\n",
    "        tokenizer=None,\n",
    "        max_length=1024,\n",
    "        subset = 1.0  # 0~1\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.lang = lang\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.LABEL2ID = {\"entailment\": 0, \"contradictory\": 1, \"neutral\": 2}\n",
    "        self.ID2LABEL = {v: k for k, v in self.LABEL2ID.items()}\n",
    "\n",
    "        if split==\"test\":\n",
    "            path = test_path.format(lang=self.lang)\n",
    "            df = self.read_dataset_tsv(path, split)\n",
    "            keep_cols = ['sentence1', 'sentence2', 'gold_label']\n",
    "            df = df[keep_cols].dropna()\n",
    "            df.rename(columns={'sentence1':'premise','sentence2':'hypo','gold_label':'label'}, inplace=True)\n",
    "            df['label'] = df['label'].replace({'contradiction': 'contradictory'})\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of ['train','dev','test']\")\n",
    "        \n",
    "        original_num = len(df)\n",
    "        if subset < 1.0:\n",
    "            n = max(1, int(len(df) * subset))\n",
    "            df = df.iloc[:n].reset_index(drop=True)\n",
    "        subset_num = len(df)\n",
    "\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        print(f\"Dataset initialized: split='{split}', lang='{lang}', total={original_num}, subset={subset}, subset_count={subset_num}\")\n",
    "\n",
    "    def read_dataset_tsv(self, path, split):\n",
    "        if split == \"train\":\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines()\n",
    "            header = lines[0].split(\"\\t\")\n",
    "            data = []\n",
    "            for i, line in enumerate(lines[1:], start=2):\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == len(header):\n",
    "                    data.append(parts)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(parts)} cols → {parts[:2]}\")\n",
    "        else:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                reader = csv.reader(f, delimiter=\"\\t\")\n",
    "                rows = list(reader)\n",
    "            header = rows[0]\n",
    "            expected_cols = len(header)\n",
    "            data = []\n",
    "            for i, row in enumerate(rows[1:], start=2):\n",
    "                if len(row) == expected_cols:\n",
    "                    data.append(row)\n",
    "                else:\n",
    "                    print(f\"skip row {i}: {len(row)} cols → {row[:2]}\")\n",
    "        return pd.DataFrame(data, columns=header)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        premise = row['premise']\n",
    "        hypo = row['hypo']\n",
    "        label = row['label']\n",
    "        if self.lang == 'zh': # de-tokenize for Chinese\n",
    "            premise = premise.replace(\" \", \"\")\n",
    "            hypo = hypo.replace(\" \", \"\")\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            prefix = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            full_text = prefix + str(self.LABEL2ID[label])\n",
    "            tokenized = self.tokenizer(\n",
    "                full_text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "            prefix_ids = self.tokenizer(prefix).input_ids\n",
    "            labels_ids = tokenized['input_ids'].clone()\n",
    "            labels_ids[:len(prefix_ids)] = -100 # Masks the prefix tokens in the labels with -100 for GPT loss computation.\n",
    "            tokenized['labels'] = labels_ids\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized\n",
    "        else:\n",
    "            text = f\"Premise: {premise}\\nHypothesis: {hypo}\\nLabel:\"\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "            tokenized['label_str'] = str(self.LABEL2ID[label])\n",
    "            return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c08fd009-1c78-41d9-bee2-53125d49097d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T10:37:11.391640Z",
     "iopub.status.busy": "2025-12-06T10:37:11.391076Z",
     "iopub.status.idle": "2025-12-06T10:37:43.659216Z",
     "shell.execute_reply": "2025-12-06T10:37:43.658737Z",
     "shell.execute_reply.started": "2025-12-06T10:37:11.391615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d28c1d1ea544d4781234502fe548cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: split='test', lang='vi', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:08<00:00, 121.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 66.70%\n",
      "Training took 0.14 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='zh', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:09<00:00, 109.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 57.00%\n",
      "Training took 0.15 minutes for all 1000 samples\n",
      "Dataset initialized: split='test', lang='es', total=1000, subset=1, subset_count=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 1000/1000 [00:07<00:00, 129.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 68.20%\n",
      "Training took 0.13 minutes for all 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Load a pretrained GPT-2 model with official weights\n",
    "model = GPT2Model.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "\n",
    "# Then continue from checkpoint if training was disrupted\n",
    "model.load_state_dict(torch.load(\"./best_model/model_multi_epoch_2.pt\"))\n",
    "model.eval()\n",
    "\n",
    "for lang in [\"vi\", \"zh\", \"es\"]:\n",
    "    \n",
    "    test_dataset = CSDataset(\n",
    "        split=\"test\",\n",
    "        lang=lang,\n",
    "        tokenizer=tokenizer,\n",
    "        subset=TEST_SUBSET\n",
    "    )\n",
    "                    \n",
    "    test_loader = DataLoader(test_dataset,shuffle=False,collate_fn=XNLIDataset.collate_fn)\n",
    "\n",
    "    start_time = time.time()\n",
    "    acc, all_preds, all_labels = evaluate_gpt2_xnli(model, tokenizer, test_loader, max_gen_length=1, device=DEVICE)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    print(f\"Training took {time_taken/60:.2f} minutes for all {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161ebb8-0fe8-4bb7-91d3-6af3f80b8584",
   "metadata": {},
   "source": [
    "## Checking model params size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "598713ac-b375-462b-a76b-39252c72df65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T05:23:06.274001Z",
     "iopub.status.busy": "2025-11-24T05:23:06.273345Z",
     "iopub.status.idle": "2025-11-24T05:23:06.287570Z",
     "shell.execute_reply": "2025-11-24T05:23:06.287124Z",
     "shell.execute_reply.started": "2025-11-24T05:23:06.273982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------+\n",
      "|                  Modules                  | Parameters |\n",
      "+-------------------------------------------+------------+\n",
      "|           word_embedding.weight           |  38597376  |\n",
      "|            pos_embedding.weight           |   786432   |\n",
      "|  gpt_layers.0.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.0.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.0.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.0.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.0.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.0.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.0.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.0.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.0.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.0.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.0.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.0.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.0.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.0.out_dense.bias        |    768     |\n",
      "|     gpt_layers.0.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.0.out_layer_norm.bias     |    768     |\n",
      "|  gpt_layers.1.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.1.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.1.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.1.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.1.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.1.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.1.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.1.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.1.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.1.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.1.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.1.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.1.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.1.out_dense.bias        |    768     |\n",
      "|     gpt_layers.1.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.1.out_layer_norm.bias     |    768     |\n",
      "|  gpt_layers.2.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.2.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.2.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.2.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.2.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.2.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.2.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.2.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.2.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.2.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.2.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.2.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.2.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.2.out_dense.bias        |    768     |\n",
      "|     gpt_layers.2.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.2.out_layer_norm.bias     |    768     |\n",
      "|  gpt_layers.3.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.3.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.3.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.3.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.3.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.3.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.3.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.3.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.3.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.3.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.3.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.3.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.3.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.3.out_dense.bias        |    768     |\n",
      "|     gpt_layers.3.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.3.out_layer_norm.bias     |    768     |\n",
      "|  gpt_layers.4.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.4.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.4.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.4.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.4.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.4.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.4.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.4.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.4.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.4.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.4.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.4.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.4.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.4.out_dense.bias        |    768     |\n",
      "|     gpt_layers.4.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.4.out_layer_norm.bias     |    768     |\n",
      "|  gpt_layers.5.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.5.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.5.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.5.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.5.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.5.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.5.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.5.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.5.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.5.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.5.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.5.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.5.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.5.out_dense.bias        |    768     |\n",
      "|     gpt_layers.5.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.5.out_layer_norm.bias     |    768     |\n",
      "|  gpt_layers.6.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.6.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.6.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.6.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.6.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.6.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.6.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.6.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.6.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.6.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.6.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.6.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.6.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.6.out_dense.bias        |    768     |\n",
      "|     gpt_layers.6.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.6.out_layer_norm.bias     |    768     |\n",
      "|  gpt_layers.7.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.7.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.7.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.7.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.7.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.7.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.7.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.7.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.7.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.7.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.7.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.7.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.7.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.7.out_dense.bias        |    768     |\n",
      "|     gpt_layers.7.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.7.out_layer_norm.bias     |    768     |\n",
      "|  gpt_layers.8.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.8.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.8.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.8.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.8.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.8.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.8.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.8.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.8.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.8.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.8.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.8.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.8.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.8.out_dense.bias        |    768     |\n",
      "|     gpt_layers.8.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.8.out_layer_norm.bias     |    768     |\n",
      "|  gpt_layers.9.self_attention.query.weight |   589824   |\n",
      "|   gpt_layers.9.self_attention.query.bias  |    768     |\n",
      "|   gpt_layers.9.self_attention.key.weight  |   589824   |\n",
      "|    gpt_layers.9.self_attention.key.bias   |    768     |\n",
      "|  gpt_layers.9.self_attention.value.weight |   589824   |\n",
      "|   gpt_layers.9.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.9.attention_dense.weight    |   589824   |\n",
      "|     gpt_layers.9.attention_dense.bias     |    768     |\n",
      "|  gpt_layers.9.attention_layer_norm.weight |    768     |\n",
      "|   gpt_layers.9.attention_layer_norm.bias  |    768     |\n",
      "|      gpt_layers.9.interm_dense.weight     |  2359296   |\n",
      "|       gpt_layers.9.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.9.out_dense.weight       |  2359296   |\n",
      "|        gpt_layers.9.out_dense.bias        |    768     |\n",
      "|     gpt_layers.9.out_layer_norm.weight    |    768     |\n",
      "|      gpt_layers.9.out_layer_norm.bias     |    768     |\n",
      "| gpt_layers.10.self_attention.query.weight |   589824   |\n",
      "|  gpt_layers.10.self_attention.query.bias  |    768     |\n",
      "|  gpt_layers.10.self_attention.key.weight  |   589824   |\n",
      "|   gpt_layers.10.self_attention.key.bias   |    768     |\n",
      "| gpt_layers.10.self_attention.value.weight |   589824   |\n",
      "|  gpt_layers.10.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.10.attention_dense.weight   |   589824   |\n",
      "|     gpt_layers.10.attention_dense.bias    |    768     |\n",
      "| gpt_layers.10.attention_layer_norm.weight |    768     |\n",
      "|  gpt_layers.10.attention_layer_norm.bias  |    768     |\n",
      "|     gpt_layers.10.interm_dense.weight     |  2359296   |\n",
      "|      gpt_layers.10.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.10.out_dense.weight      |  2359296   |\n",
      "|        gpt_layers.10.out_dense.bias       |    768     |\n",
      "|    gpt_layers.10.out_layer_norm.weight    |    768     |\n",
      "|     gpt_layers.10.out_layer_norm.bias     |    768     |\n",
      "| gpt_layers.11.self_attention.query.weight |   589824   |\n",
      "|  gpt_layers.11.self_attention.query.bias  |    768     |\n",
      "|  gpt_layers.11.self_attention.key.weight  |   589824   |\n",
      "|   gpt_layers.11.self_attention.key.bias   |    768     |\n",
      "| gpt_layers.11.self_attention.value.weight |   589824   |\n",
      "|  gpt_layers.11.self_attention.value.bias  |    768     |\n",
      "|    gpt_layers.11.attention_dense.weight   |   589824   |\n",
      "|     gpt_layers.11.attention_dense.bias    |    768     |\n",
      "| gpt_layers.11.attention_layer_norm.weight |    768     |\n",
      "|  gpt_layers.11.attention_layer_norm.bias  |    768     |\n",
      "|     gpt_layers.11.interm_dense.weight     |  2359296   |\n",
      "|      gpt_layers.11.interm_dense.bias      |    3072    |\n",
      "|       gpt_layers.11.out_dense.weight      |  2359296   |\n",
      "|        gpt_layers.11.out_dense.bias       |    768     |\n",
      "|    gpt_layers.11.out_layer_norm.weight    |    768     |\n",
      "|     gpt_layers.11.out_layer_norm.bias     |    768     |\n",
      "|            pooler_dense.weight            |   589824   |\n",
      "|             pooler_dense.bias             |    768     |\n",
      "|          final_layer_norm.weight          |    768     |\n",
      "|           final_layer_norm.bias           |    768     |\n",
      "+-------------------------------------------+------------+\n",
      "Total Trainable Params: 125030400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125030400"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
